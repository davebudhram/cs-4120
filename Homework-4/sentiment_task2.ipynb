{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 2\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names: Dave Budhram(4120) and Akshay Dupuguntla(4120) (Write these in every notebook you submit. For each partner, write down whether you are a 4120 or a 6120 student.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Train a Naive Bayes Model (30 points)\n",
    "----\n",
    "\n",
    "Using `nltk`'s `NaiveBayesClassifier` class, train a Naive Bayes classifier using a Bag of Words as features.\n",
    "https://www.nltk.org/_modules/nltk/classify/naivebayes.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davebudhram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# our utility functions\n",
    "# RESTART your jupyter notebook kernel if you make changes to this file\n",
    "import sentiment_utils as sutils\n",
    "\n",
    "# nltk for Naive Bayes and metrics\n",
    "import nltk\n",
    "import nltk.classify.util\n",
    "from nltk.metrics.scores import (precision, recall, f_measure, accuracy)\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# some potentially helpful data structures from collections\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# so that we can make plots\n",
    "import matplotlib.pyplot as plt\n",
    "# if you want to use seaborn to make plots\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "\n",
    "# x: list of tokens for each review\n",
    "# y: list of label value for each review\n",
    "X_train, y_train = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "X_dev, y_dev = sutils.generate_tuples_from_file(DEV_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Dev Example - actual: 0 pred: 0\n",
      "Second Dev Example - actual: 1 pred: 1\n"
     ]
    }
   ],
   "source": [
    "# set up a sentiment classifier using NLTK's NaiveBayesClassifier and \n",
    "# a bag of words as features\n",
    "# take a look at the function in lecture notebook 7 (feel free to copy + paste that function)\n",
    "# the nltk classifier expects a dictionary of features as input where the key is the feature name\n",
    "# and the value is the feature value\n",
    "\n",
    "# need to return a dict to work with the NLTK classifier\n",
    "# Possible problem for students: evaluate the difference \n",
    "# between using binarized features and using counts (non binarized features)\n",
    "def word_feats(words):\n",
    "    \"\"\"\n",
    "    This function converts a list of words so that they are featurized\n",
    "    for nltk's format for bag-of-words\n",
    "    Parameters:\n",
    "    words - list of words where each element is a single word \n",
    "    Returns: dict mapping every word to True\n",
    "    \"\"\"\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "\n",
    "\n",
    "# set up & train a sentiment classifier using NLTK's NaiveBayesClassifier and\n",
    "# classify the first example in the dev set as an example\n",
    "# make sure your output is well-labeled\n",
    "# Should take < 10 sec to train (on Felix's computer this takes 0.5 sec)\n",
    "\n",
    "# Create list of tuple(dict(review tokens), label)\n",
    "train_set = []\n",
    "for review, label in zip(X_train, y_train):\n",
    "    train_set.append((word_feats(review), label))\n",
    "\n",
    "# Create classifier from train data\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Classify first example in dev set\n",
    "dev_sentiment = classifier.classify(word_feats(X_dev[0]))\n",
    "print(\"First Dev Example - actual: \" + str(y_dev[0]) + \" pred: \" + str(dev_sentiment))\n",
    "\n",
    "# test to make sure that you can train the classifier and use it to classify a new example\n",
    "dev_sentiment = classifier.classify(word_feats(X_dev[1]))\n",
    "print(\"Second Dev Example - actual: \" + str(y_dev[1]) + \" pred: \" + str(dev_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ConfusionMatrix.precision() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/davebudhram/Documents/Northeastern/CS4120/Homework-4/sentiment_task2.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/Homework-4/sentiment_task2.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m refsets, testsets \u001b[39m=\u001b[39m binaryClassify(predicted_labels, true_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/Homework-4/sentiment_task2.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m cm \u001b[39m=\u001b[39m ConfusionMatrix(dev_tups_y, predicted_labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/Homework-4/sentiment_task2.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(cm\u001b[39m.\u001b[39;49mprecision())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/Homework-4/sentiment_task2.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Calculate precision, recall, and F1 score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/Homework-4/sentiment_task2.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# print(\"Accuracy:\", acc)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/Homework-4/sentiment_task2.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# print('pos precision:', precision(refsets[1], testsets[1]))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/Homework-4/sentiment_task2.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/Homework-4/sentiment_task2.ipynb#X10sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# takes approximately 3.5sec to run on Felix's computer\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ConfusionMatrix.precision() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "# Using the provided dev set, evaluate your model with precision, recall, and f1 score as well as accuracy\n",
    "# You may use nltk's implemented `precision`, `recall`, `f_measure`, and `accuracy` functions\n",
    "# (make sure to look at the documentation for these functions!)\n",
    "# you will be creating a similar graph for logistic regression and neural nets, so make sure\n",
    "# you use functions wisely so that you do not have excessive repeated code\n",
    "# write any helper functions you need in sentiment_utils.py (functions that you'll use in your other notebooks as well)\n",
    "\n",
    "# Classify each dev_tup in the the movie reviews dev file. We need to call extraword_featsct_features \n",
    "# on each one before adding it to a list\n",
    "predicted_labels = [classifier.classify(word_feats(document)) for document in X_dev]\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = accuracy(predicted_labels, y_dev)\n",
    "\n",
    "def binaryClassify(predicted_labels: [], true_labels: []):\n",
    "  refsets = defaultdict(set) # true\n",
    "  testsets = defaultdict(set) # pred\n",
    "  # Create list of tuples pairing predicated value and true values\n",
    "  labels = [(pred, true) for pred, true in zip(predicted_labels, true_labels)]\n",
    "  for i, (pred, true) in enumerate(labels):\n",
    "    refsets[true].add(i)\n",
    "    testsets[pred].add(i)\n",
    "  return refsets, testsets\n",
    "refsets, testsets = binaryClassify(predicted_labels, y_dev)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "print(\"Accuracy:\", acc)\n",
    "print('pos precision:', precision(refsets[1], testsets[1]))\n",
    "print( 'pos recall:', recall(refsets[1], testsets[1]))\n",
    "# print( 'pos F-1:', f_measure(refsets[1], testsets[1]))\n",
    "# print( 'neg precision:', precision(refsets[0], testsets[0]))\n",
    "# print( 'neg recall:', recall(refsets[0], testsets[0]))\n",
    "# print( 'neg F-1:', f_measure(refsets[0], testsets[0]))\n",
    "# Calculate precision, recall, and F1 score\n",
    "# precision = precision(cm)\n",
    "# recall = recall(cm)\n",
    "# f1_score = f_measure(precision, recall)\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"Precision: {precision:.2f}\")\n",
    "# print(f\"Recall: {recall:.2f}\")\n",
    "# print(f\"F1 Score: {f1_score:.2f}\")\n",
    "\n",
    "\n",
    "# create a graph of your classifier's performance on the dev set as a function of the amount of training data\n",
    "# the x-axis should be the amount of training data (as a percentage of the total training data)\n",
    "# the y-axis should be the performance of the classifier on the dev set\n",
    "# the graph should have 4 lines, one for each of precision, recall, f1, and accuracy\n",
    "# the graph should have a legend, title, and axis labels\n",
    "\n",
    "# takes approximately 3.5sec to run on Felix's computer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model using both a __binarized__ and a __multinomial__ BoW. Use whichever one gives you a better final f1 score on the dev set to produce your graphs.\n",
    "\n",
    "- f1 score binarized: __YOUR ANSWER HERE__\n",
    "- f1 score multinomial: __YOUR ANSWER HERE__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
