{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 3\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names: Dave Budhram(4120) and Akshay Dupuguntla(4120) (Write these in every notebook you submit. For each partner, write down whether you are a 4120 or a 6120 student.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Train a Logistic Regression Model (20 points)\n",
    "----\n",
    "\n",
    "Using `sklearn`'s implementation of `LogisticRegression`, conduct a similar analysis on the performance of a Logistic Regression classifier on the provided data set.\n",
    "\n",
    "Using the `time` module, you'll compare and contrast how long it takes your home-grown BoW vectorizing function vs. `sklearn`'s `CountVectorizer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.metrics.scores import (precision, recall, f_measure, accuracy)\n",
    "from collections import Counter\n",
    "import time\n",
    "import sentiment_utils as sutils\n",
    "from sentiment_utils import convert2DArray\n",
    "from sentiment_utils import CustomCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\"\n",
    "\n",
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "X_train, y_train = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "X_dev, y_dev = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "\n",
    "# some variables you may want to use\n",
    "BINARIZED = True\n",
    "USE_COUNT_VECTORIZER = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Write the functions needed (here or in sentiment_utils.py) to create vectorized BoW representations\n",
    "# of your data. We recommend starting with a multinomial BoW representation.\n",
    "# Each training example should be represented as a sparse vector.\n",
    "\n",
    "X_train_strs = convert2DArray(X_train)\n",
    "X_dev_strs = convert2DArray(X_dev) \n",
    "count_vectorizer = CountVectorizer(binary=False)\n",
    "count_vectorizer.fit(X_train_strs)\n",
    "X_train_bow = count_vectorizer.transform(X_train_strs)\n",
    "X_dev_bow = count_vectorizer.transform(X_dev_strs)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_bow, y_train)\n",
    "y_pred = clf.predict(X_dev_bow)\n",
    "\n",
    "accuracy_val = accuracy(y_dev, y_pred)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "print(\"Accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "customCountVectorizer = CustomCountVectorizer(binary=False)\n",
    "customCountVectorizer.fit(X_train)\n",
    "X_train_bow = customCountVectorizer.transform(X_train)\n",
    "X_dev_bow = customCountVectorizer.transform(X_dev)\n",
    "# print(matrix[0])\n",
    "# print(len(customCountVectorizer.vocab))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_bow, y_train)\n",
    "y_pred = clf.predict(X_dev_bow)\n",
    "\n",
    "accuracy_val = accuracy(y_dev, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# all_reviews = []\n",
    "# for review in train_tups_x:\n",
    "#   all_reviews.extend(review)\n",
    "# vocab = set(all_reviews)\n",
    "# count = Counter(all_reviews)\n",
    "# print(len(set(count.keys())))\n",
    "# vocab = count.keys()\n",
    "# print(len(count)) # number of words \n",
    "# for review in train_tups_x:\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took: 0.3790550231933594 seconds\n",
      "Custom Count Vectorize Vocab Size: 30705\n"
     ]
    }
   ],
   "source": [
    "# how much time does it take to featurize the all data with your implementation?\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "custom_count_vectorizer = CustomCountVectorizer(binary=False)\n",
    "custom_count_vectorizer.fit(X_train)\n",
    "X_train_bow = custom_count_vectorizer.transform(X_train)\n",
    "X_dev_bow = custom_count_vectorizer.transform(X_dev)\n",
    "\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds\")\n",
    "print(\"Custom Count Vectorize Vocab Size:\", len(custom_count_vectorizer.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took: 0.3257288932800293 seconds\n",
      "Sklearn CountVectorizer Vocab Size: 22596\n"
     ]
    }
   ],
   "source": [
    "# how much time does it take to featurize the all data with sklearn's CountVectorizer?\n",
    "\n",
    "X_train_strs = convert2DArray(X_train)\n",
    "X_dev_strs = convert2DArray(X_dev) \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "count_vectorizer = CountVectorizer(binary=False)\n",
    "count_vectorizer.fit(X_train_strs)\n",
    "X_train_bow = count_vectorizer.transform(X_train_strs)\n",
    "# X_dev_bow = count_vectorizer.transform(X_dev_strs)\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds\")\n",
    "print(\"Sklearn CountVectorizer Vocab Size:\", len(count_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How big is your vocabulary using your vectorization function(s)? 30705\n",
    "2. How big is your vocabulary using the `CountVectorizer`? 22596\n",
    "\n",
    "The difference in vocabulary is primarily attributed to the fact that our custom count vectorizer does not convert words to lowercase before adding them to the vocabulary. This means that words in different cases, such as \"Word\" and \"word,\" are treated as separate terms and are included in the vocabulary as distinct features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  write any code you need analyze the relative sparsity of your vectorized representations of the data\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Print out the average % of entries that are zeros in each vector in the vectorized training data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the provided dev set, evaluate your model with precision, recall, and f1 score as well as accuracy\n",
    "# You may use nltk's implemented `precision`, `recall`, `f_measure`, and `accuracy` functions\n",
    "# (make sure to look at the documentation for these functions!)\n",
    "# you will be creating a similar graph for logistic regression and neural nets, so make sure\n",
    "# you use functions wisely so that you do not have excessive repeated code\n",
    "# write any helper functions you need in sentiment_utils.py (functions that you'll use in your other notebooks as well)\n",
    "\n",
    "\n",
    "# create a graph of your classifier's performance on the dev set as a function of the amount of training data\n",
    "# the x-axis should be the amount of training data (as a percentage of the total training data)\n",
    "# the y-axis should be the performance of the classifier on the dev set\n",
    "# the graph should have 4 lines, one for each of precision, recall, f1, and accuracy\n",
    "# the graph should have a legend, title, and axis labels\n",
    "\n",
    "# takes approx 30 sec on Felix's computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the following 4 combinations to determine which has the best final f1 score for your Logistic Regression model:\n",
    "- your vectorized features, multinomial: __enter your final f1 score here__\n",
    "- CountVectorizer features, multinomial: __enter your final f1 score here__\n",
    "- your vectorized features, binarized: __enter your final f1 score here__\n",
    "- CountVectorizer features, binarized: __enter your final f1 score here__\n",
    "\n",
    "Produce your graph(s) for the combination with the best final f1 score.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6120 REQUIRED\n",
    "----\n",
    "\n",
    "Find the top 100 most important features to your Logistic Regression classifier when using 100% of the training data. To access the weights of your model, you can access the `model.coef_` attribute. You'll want to use a `StandardScalar` preprocessor. This will help us deal with the fact that we expect counts of certain words to be higher (e.g. stop words).\n",
    "\n",
    "To find the importance of a feature, calculate the absolute value of each weight in the model, then order your features according to the absolute values of these weights. The feature with the heighest absolute value weight has the most importance.\n",
    "\n",
    "Use __your__ (not CountVectorizer) multinomial vectors for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# train a model on the scaled inputs\n",
    "# This takes Felix's computer about 6.5 sec to run\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the top 20 most informative features according to this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-evalaute your LR model with inputs that have been filtered to only use the top 500 most informative features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the same graph as before, but with the filtered inputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
