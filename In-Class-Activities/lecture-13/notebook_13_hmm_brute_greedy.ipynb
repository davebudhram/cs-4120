{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 13: HMMs (part 2), decoding (brute force, greedy)\n",
    "===============\n",
    "\n",
    "10/30/2023, CS 4/6120 Natural Language Processing, Muzny\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: brute force for \"Janet will back the bill\"\n",
    "-----------------\n",
    "\n",
    "The tables and examples here correspond to section 8.4.6 in the textbook and tables 8.12 and 8.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NNP': 0.2767, 'MD': 0.0006, 'VB': 0.0031, 'JJ': 0.0453, 'NN': 0.0449, 'RB': 0.051, 'DT': 0.2026}\n",
      "{'RB', 'NN', 'JJ', 'NNP', 'VB', 'MD', 'DT'}\n"
     ]
    }
   ],
   "source": [
    "# First, set up our model's paramters\n",
    "keys = \"NNP MD VB JJ NN RB DT\".split()  # these are our states\n",
    "values = \"0.2767 0.0006 0.0031 0.0453 0.0449 0.0510 0.2026\".split()  # this is pi\n",
    "pi = {keys[i]: float(values[i]) for i in range(len(keys))}\n",
    "Q = set(keys)\n",
    "print(pi)\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NNP': Counter({'NNP': 0.3777, 'NN': 0.0584, 'MD': 0.011, 'RB': 0.009, 'JJ': 0.0084, 'DT': 0.0025, 'VB': 0.0009}), 'MD': Counter({'VB': 0.7968, 'RB': 0.1698, 'DT': 0.0041, 'NNP': 0.0008, 'NN': 0.0008, 'JJ': 0.0005, 'MD': 0.0002}), 'VB': Counter({'DT': 0.2231, 'JJ': 0.0837, 'NN': 0.0615, 'RB': 0.0514, 'NNP': 0.0322, 'VB': 0.005, 'MD': 0.0005}), 'JJ': Counter({'NN': 0.4509, 'JJ': 0.0733, 'NNP': 0.0366, 'RB': 0.0036, 'DT': 0.0036, 'MD': 0.0004, 'VB': 0.0001}), 'NN': Counter({'NN': 0.1216, 'RB': 0.0177, 'MD': 0.0176, 'NNP': 0.0096, 'JJ': 0.0086, 'DT': 0.0068, 'VB': 0.0014}), 'RB': Counter({'JJ': 0.1012, 'VB': 0.1011, 'RB': 0.0728, 'DT': 0.0479, 'NN': 0.012, 'MD': 0.0102, 'NNP': 0.0068}), 'DT': Counter({'NN': 0.4744, 'JJ': 0.2157, 'NNP': 0.1147, 'RB': 0.0102, 'MD': 0.0021, 'DT': 0.0017, 'VB': 0.0002})}\n"
     ]
    }
   ],
   "source": [
    "A_table = [\"0.3777 0.0110 0.0009 0.0084 0.0584 0.0090 0.0025\".split(),\\\n",
    "           \"0.0008 0.0002 0.7968 0.0005 0.0008 0.1698 0.0041\".split(),\\\n",
    "          \"0.0322 0.0005 0.0050 0.0837 0.0615 0.0514 0.2231\".split(),\\\n",
    "          \"0.0366 0.0004 0.0001 0.0733 0.4509 0.0036 0.0036\".split(),\\\n",
    "          \"0.0096 0.0176 0.0014 0.0086 0.1216 0.0177 0.0068\".split(),\\\n",
    "          \"0.0068 0.0102 0.1011 0.1012 0.0120 0.0728 0.0479\".split(),\\\n",
    "          \"0.1147 0.0021 0.0002 0.2157 0.4744 0.0102 0.0017\".split()]\n",
    "\n",
    "# p(t_i| t_{i - 1})\n",
    "\n",
    "# to access a given probability, we'll enter\n",
    "# \"state\" and \"previous state\" in \"reverse order\"\n",
    "# consistent with rows being the previous/given state and\n",
    "# columns being the current one\n",
    "# A[t_i - 1][t_i]\n",
    "A = {keys[i]:Counter({keys[j]:float(A_table[i][j]) for j in range(len(keys))}) for i in range(len(keys))}\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(VB|MD): 0.7968\n",
      "p(NN|JJ) 0.0086\n"
     ]
    }
   ],
   "source": [
    "print(\"p(VB|MD):\", A[\"MD\"][\"VB\"])\n",
    "\n",
    "# Any experiments that you want here\n",
    "print(\"p(NN|JJ)\", A[\"NN\"][\"JJ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the probability of the next word being a noun (`NN`) if we just saw an adjective (`JJ`)? 0.0086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p(w_i| t_i)\n",
    "\n",
    "# to access a given probability, we'll enter\n",
    "# \"state\" and \"word\" in \"reverse order\"\n",
    "# consistent with rows being the given state and\n",
    "# columns being the current word\n",
    "# B[t_i][w_i]\n",
    "B = {\"NNP\": Counter({\"Janet\":0.000032, \"the\": 0.000048}),\\\n",
    "     \"MD\": Counter({\"will\":0.308431}),\\\n",
    "    \"VB\": Counter({\"will\":0.000028, \"back\": 0.000672, \"bill\":0.000028}),\\\n",
    "    \"JJ\": Counter({\"back\":0.00034}),\\\n",
    "    \"NN\": Counter({\"will\":0.0002, \"back\": 0.000223, \"bill\": 0.002337}),\\\n",
    "    \"RB\": Counter({\"back\":0.010446}),\\\n",
    "    \"DT\": Counter({\"the\":0.506099})}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(will|MD): 0.308431\n",
      "p(back|VB): 0.000672\n"
     ]
    }
   ],
   "source": [
    "print(\"p(will|MD):\", B[\"MD\"][\"will\"])\n",
    "\n",
    "# Any experiments that you want here\n",
    "print(\"p(back|VB):\", B[\"VB\"][\"back\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the probability of the word being \"back\" if the tag is verb (`VB`)? 0.000672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so that we can enumerate all the sequences\n",
    "import itertools\n",
    "\n",
    "\n",
    "def brute_force(o: list, pi: dict, A: dict, B: dict, Q: set) -> (tuple, float):\n",
    "    \"\"\"\n",
    "    Enumerate all possible tag sequences and choose the best one according to\n",
    "    the passed in transition and emission tables.\n",
    "    \n",
    "    params:\n",
    "    o - a list of tokens (observations)\n",
    "    pi - a dictionary of the initial probabilities for all states (p(state | <s>))\n",
    "    A - a dictionary of transition probabilities where previous state maps to current state (p(t_i| t_i - 1))\n",
    "    B - a dictionary of emission probabilities where state maps to words (p(w_i | t_i))\n",
    "    Q - a set of states (strings)\n",
    "    \n",
    "    return:\n",
    "    max_seq - a tuple of the best sequence of tags for the observation\n",
    "    max_prob - a float of the final calculated probability for that sequence of tags\n",
    "    \"\"\"\n",
    "    # generate the sequences\n",
    "    # this is the cartesian product\n",
    "    sequences = list(itertools.product(Q, repeat=len(o)))\n",
    "    print(\"Number of states:\", len(Q))\n",
    "    print(\"Number of words:\", len(o))\n",
    "    print(\"Number of sequences:\", len(Q) ** len(o))\n",
    "    print(\"I have generated:\", len(sequences))\n",
    "    \n",
    "    probabilities = {}\n",
    "    for seq in sequences:\n",
    "        \n",
    "        # TODO: evaluate the probability of this sequence\n",
    "\n",
    "        # you'll probably want to loop over the sequence\n",
    "        prob = 1\n",
    "        for t in range(len(seq)):\n",
    "            state = seq[t]\n",
    "            word = o[t]\n",
    "            emission = B[state][word]\n",
    "            if t == 0:\n",
    "                # initial probability\n",
    "                # you'll need something different for the else statement\n",
    "                # you will be using A instead of pi in the else statement\n",
    "                initial_prob = pi[state]\n",
    "                # print(\"p(will|MD):\", B[\"MD\"][\"will\"])\n",
    "             \n",
    "                prob *= initial_prob * emission\n",
    "            else:\n",
    "                # you need to implement the else-case\n",
    "                \n",
    "            \n",
    "                # you'll need the numbers:\n",
    "                # emission: P(w_i | t_i)\n",
    "                # pi (if t is 0)\n",
    "                # transition: p(t_i | t_i - 1) ( if t is > 0)\n",
    "                transition = A[seq[t-1]][state]\n",
    "                prob = transition * emission\n",
    "\n",
    "                # then you'll need to accumulate your product\n",
    "            \n",
    "        # finally, link this probability to this sequence\n",
    "        probabilities[seq] = prob\n",
    "        \n",
    "    # find the max probability\n",
    "    max_prob = max(probabilities.values())\n",
    "    # find the maximum sequence (the argmax)\n",
    "    max_seq = max(probabilities, key=probabilities.get, default=())\n",
    "    return (max_seq, max_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 7\n",
      "Number of words: 3\n",
      "Number of sequences: 343\n",
      "I have generated: 343\n",
      "(('RB', 'DT', 'NN'), 0.0011086728)\n",
      "That took:  0.0005528926849365234\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# do a few experiments to get to know the time \n",
    "# and how many sequences are being considered here\n",
    "example = \"back the bill\".split() \n",
    "start = time.time()\n",
    "print(brute_force(example, pi, A, B, Q))\n",
    "end = time.time()\n",
    "print(\"That took: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How long does it take to perform __brute force__ decoding with a set of 7 states and 3 words (e.g. \"back the bill\") on your machine? __YOUR ANSWER HERE__\n",
    "\n",
    "3. How long does it take to perform __brute force__ decoding with a set of 7 states and 5 words on your machine? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: greedy approach for \"Janet will back the bill\"\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(o: list, pi: dict, A: dict, B: dict, Q: set) -> (tuple, float):\n",
    "    \"\"\"\n",
    "    Greedily decode the sequence of tags to go along with a sequence of words\n",
    "    \n",
    "    params:\n",
    "    o - a list of tokens (observations)\n",
    "    pi - a dictionary of the initial probabilities for all states (p(state | <s>))\n",
    "    A - a dictionary of transition probabilities where previous state maps to current state (p(t_i| t_i - 1))\n",
    "    B - a dictionary of emission probabilities where state maps to words (p(w_i | t_i))\n",
    "    Q - a set of states (strings)\n",
    "    \n",
    "    return:\n",
    "    max_seq - a tuple of the best sequence of tags for the observation\n",
    "    max_prob - a float of the final calculated probability for that sequence of tags\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    greedy_seq = []\n",
    "    for t in range(len(o)):\n",
    "        # current observation/word\n",
    "        o_t = o[t]\n",
    "        word = o[t]\n",
    "        \n",
    "        probabilities = {}\n",
    "        # consider every state\n",
    "        for state in Q:\n",
    "            \n",
    "            # P(w_i | t_i)\n",
    "            # TODO: get the emission probability for the word given the state\n",
    "            emission =  B[state][word]\n",
    "        \n",
    "            # see if we're considering the first word\n",
    "            if t == 0:\n",
    "                prob = pi[state] * emission\n",
    "                probabilities[state] = prob\n",
    "            else:\n",
    "                # consider ONLY THE BEST previous state\n",
    "                # as the state that we came from \n",
    "                # (this is what makes this strategy different than \n",
    "                # the Viterbi algorithm, a dynamic programming approach)\n",
    "                # TODO: get the appropriate transition probability for this state\n",
    "                transition = A[greedy_seq[-1]][state]\n",
    "                    \n",
    "                # calculate the new probability\n",
    "                # TODO FILL THIS IN\n",
    "                prob = emission * transition\n",
    "                probabilities[state] = prob\n",
    "                \n",
    "        # choose the best state for the prev state\n",
    "        max_prob = max(probabilities.values())\n",
    "        # get the argmax\n",
    "        max_state = max(probabilities, key=probabilities.get, default=())\n",
    "        # build up our sequence\n",
    "        greedy_seq.append(max_state)\n",
    "        \n",
    "    return (tuple(greedy_seq), max_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('NNP', 'MD', 'RB', 'DT', 'NN'), 0.0011086728)\n",
      "That took:  0.00012803077697753906\n"
     ]
    }
   ],
   "source": [
    "# do a few experiments to get to know the time \n",
    "# and how many sequences are being considered here\n",
    "example = \"Janet will back the bill\".split() \n",
    "start = time.time()\n",
    "print(greedy(example, pi, A, B, Q))\n",
    "end = time.time()\n",
    "print(\"That took: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How does the time of your `greedy` approach compare to the `brute_force` approach? __YOUR ANSWER HERE__\n",
    "\n",
    "6. How about the calculated probability of the final sequence of tags chosen? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Using `nltk` to tag\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(\"Janet will back the bill\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Does `nltk` agree with your brute force or greedy strategies? __YOUR ANSWER HERE__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
