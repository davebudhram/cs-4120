{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 7:  Naive Bayes Scavenger Hunt\n",
    "===============\n",
    "\n",
    "10/2/2023, CS 4120 Natural Language Processing, Muzny\n",
    "\n",
    "Today, you'll be building investigating three Naive Bayes classifiers that have already been built and trained for you. Your mission is to determine which classifier is which. They are all binary classifiers.\n",
    "\n",
    "\n",
    "- one of these classifiers is an authorship attributor (the two labels **do** correspond to two specific authors)\n",
    "- one of these classifiers is a language identifier (the two labels **do** correspond to two specific languages)\n",
    "- one of these classifiers is a sentiment analyser (the two labels **do** correspond to positive and negative)\n",
    "\n",
    "Remember, for a given new, unlabeled document, they will calculate:\n",
    "\n",
    "$$ P(feature_1, feature_2, feature_3, ..., feature_n | c)P(c)$$\n",
    "\n",
    "Where the features for a document are a \"bag of words\" and $c$ is a candidate class. They then select the class that has the highest probability to be the actual label of the new document.\n",
    "\n",
    "\n",
    "Supporting files\n",
    "-------------------\n",
    "(Download these from Canvas)\n",
    "1. `classifier1.pickle`\n",
    "2. `classifier2.pickle`\n",
    "3. `classifier3.pickle`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Which Classifier is Which?\n",
    "-------------------------\n",
    "We have given you 3 Naïve Bayes classifiers. All three of these are binary classifiers that choose between the label '0' or '1' (these are strings). __They all also use a bag-of-words as features.__\n",
    "\n",
    "\n",
    "Your first job is to conduct experiments to determine two things:\n",
    "1. Which classifier is which?\n",
    "2. What specific classes do you believe that they are choosing between? (what are better labels for each classifier than '0' and '1'?)\n",
    "    1. Note: this is a __difficult__ task, especially for authorship attribution. It is of utmost importance that you consider the particular data set that they might have been trained on. They were all trained using some of [nltk's available corpora](http://www.nltk.org/nltk_data/).\n",
    "        1. For authorship attribution, try to determine the style of text that the two classes are looking for, but don't spend more than 5 - 10 minutes on this task. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your trained classifiers from pickled files\n",
    "# (we've already trained your classifiers for you)\n",
    "import pickle\n",
    "#import nltk  # not necessary, but you can uncomment if you want\n",
    "\n",
    "# add more imports here as you would like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_feats(words):\n",
    "    \"\"\"\n",
    "    This function converts a list of words so that they are featurized\n",
    "    for nltk's format for bag-of-words\n",
    "    Parameters:\n",
    "    words - list of words where each element is a single word \n",
    "    Returns: dict mapping every word to True\n",
    "    \"\"\"\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "f = open('classifier1.pickle', 'rb')\n",
    "classifier1 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('classifier2.pickle', 'rb')\n",
    "classifier2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('classifier3.pickle', 'rb')\n",
    "classifier3 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# in a list, if you find that helpful to use\n",
    "classifiers = [classifier1, classifier2, classifier3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0', '1'])\n",
      "0.6325082240556184\n",
      "0.3674917759443814\n",
      "0\n",
      "\n",
      "dict_keys(['1', '0'])\n",
      "0.9999999621751425\n",
      "3.7824855426585115e-08\n",
      "0\n",
      "\n",
      "dict_keys(['0', '1'])\n",
      "0.867841315914037\n",
      "0.1321586840859627\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here's an example of how to run a test sentence through the classifiers\n",
    "# edit at your leisure\n",
    "test = \"this is a test sentence\"\n",
    "# you can either split on whitespace or use nltk's word_tokenize\n",
    "featurized = word_feats(test.split()) \n",
    "\n",
    "for classifier in classifiers:\n",
    "    print(classifier.prob_classify(featurized).samples())  # will tell you what classes are available\n",
    "    print(classifier.prob_classify(featurized).prob('0'))  # get the probability for class '0'\n",
    "    print(classifier.prob_classify(featurized).prob('1'))  # get the probability for class '1'\n",
    "    print(classifier.classify(featurized))  # just get the label that it wants to assign\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0', '1'])\n",
      "0.5\n",
      "0.5\n",
      "1\n",
      "\n",
      "dict_keys(['1', '0'])\n",
      "0.47432905484247373\n",
      "0.5256709451575262\n",
      "1\n",
      "\n",
      "dict_keys(['0', '1'])\n",
      "0.05203965097074141\n",
      "0.9479603490292587\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: put in as many experiments as you'd like here (and feel free to add more cells as needed)\n",
    "# we recommend testing a variety of sentences. You can make these up or get them from sources\n",
    "# on the web\n",
    "sentence = \"Hamlet\"\n",
    "# you can either split on whitespace or use nltk's word_tokenize\n",
    "featurized = word_feats(sentence.split()) \n",
    "\n",
    "for classifier in classifiers:\n",
    "    print(classifier.prob_classify(featurized).samples())  # will tell you what classes are available\n",
    "    print(classifier.prob_classify(featurized).prob('0'))  # get the probability for class '0'\n",
    "    print(classifier.prob_classify(featurized).prob('1'))  # get the probability for class '1'\n",
    "    print(classifier.classify(featurized))  # just get the label that it wants to assign\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the questions outlined at the beginning of this task here:\n",
    "\n",
    "1. Which classifier is which?\n",
    "    1. classifier1 is Sentiment\n",
    "    1. classifier2 is Language\n",
    "    1. classifier3 is Authorship\n",
    "2. What specific classes do you believe that they are choosing between?\n",
    "    1. classifier1's '0' label should be Negative and its '1' label should be Positive\n",
    "    1. classifier2's '0' label should be English and its '1' label should be Spanish\n",
    "    1. classifier3's '0' label should be Shake and its '1' label should be Jane Austen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions about Naïve Bayes classifiers in general:\n",
    "\n",
    "1. If a naïve bayes classifier for sentiment was trained on a certain corpus—hand labeled sentences from Shakespeare's plays, for instance—using BoW as features, but then evaluated on a test set of IMDB movie reviews, what do you think its performance might be? __YOUR ANSWER HERE__\n",
    "\n",
    "2. Justify your answer by comparing/contrasting with other possible test sets that you might evaluate this classifier against. __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Calculating Naive Bayes Probabilities\n",
    "----\n",
    "\n",
    "Given the following training data and assuming that you are using a Bag of Words as your features what is the value of $P(c = 0 | x = \\texttt{I have two dogs and one fluffy cat})$ ? (don't take the argmax here—this is the innards of equation 4.8/4.9 from the text)\n",
    "\n",
    "\n",
    "Use a multinomial (in terms of features) naive bayes classifier with laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the size of your vocabulary here\n",
    "# you'll also need the number of words for each class\n",
    "# (use the len() function to get this)\n",
    "words_0 = \"cats are good that is one fuzzy cat a fuzzy cat is not a fluffy dog\".split()\n",
    "words_1 = \"dogs are happy I like my fluffy dog\".split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just do the math by hand in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll do it in a more programmatic fashion\n",
    "x = \"I have two dogs and one fluffy cat\".split()\n",
    "\n",
    "# calculate the priors from the training data in this cell\n",
    "N_0 = 3\n",
    "N_1 = 2\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(\"Probability of class 0 for:\", x)\n",
    "# print out the probability here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate the probability of class 1 as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that you can change the text of x to whatever \n",
    "# you want to get new (correct) probabilities\n",
    "\n",
    "\n",
    "# bonus challenge: make this into a function!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
