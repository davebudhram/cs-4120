{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ1qJahf6Cmi"
   },
   "source": [
    "Lecture 11: Neural Nets + Word Embeddings\n",
    "===============\n",
    "\n",
    "10/23/2023, CS 4/6120 Natural Language Processing, Muzny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5ZpbRZA6Cml"
   },
   "source": [
    "Task 1: Is a single word positive or negative?\n",
    "-----------------\n",
    "\n",
    "We'll start by trying to train a neural net to recognize if a word is positive or negative based on its word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q21wjQAT6Cmm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# if you are inspired to graph things :)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# seed random number generation so that you can \n",
    "# track the same numbers as each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xXxJ2TkCh8s",
    "outputId": "c584acbe-2f4e-4b51-8953-ce9082a02fc8"
   },
   "outputs": [],
   "source": [
    "# if you are running on google colab, you'll want the following code\n",
    "# so we can load data in\n",
    "# (upload the train/dev files to your google drive first)\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5jO58c9oo4d"
   },
   "source": [
    "You'll need to upload the data files that you're reading from to your google drive so that you can access them after it's mounted and given permissions. You can find find your drive files under `/content/drive/MyDrive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeYPvh7c6Cmn"
   },
   "outputs": [],
   "source": [
    "# we'll get our word embeddings from the gensim package\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i47kGcDL6Cmn",
    "outputId": "bcaabe72-ac53-4841-aaa0-7b13f98f6525"
   },
   "outputs": [],
   "source": [
    "# go get some pre-trained word embeddings with 50 dimensions\n",
    "# glove refers to a different algorithm for creating the vectors\n",
    "# the resulting vectors follow the same structure:\n",
    "# dense vectors representing words in n-dimensional space\n",
    "\n",
    "# we're using this one because it's the smallest\n",
    "# set available pre-trained from gensim\n",
    "# it is still 66 MB!\n",
    "word2vec_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3j7JQ3yW6Cmo",
    "outputId": "02126125-bcca-4a77-f418-46443a6faf07"
   },
   "outputs": [],
   "source": [
    "# accessing a specific word vector\n",
    "print(word2vec_vectors['computer'])\n",
    "\n",
    "# TODO: measure the similarity between a few of the vectors in this set\n",
    "# documentation for vectors overall:\n",
    "# https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors\n",
    "# documentation for specific methods:\n",
    "# https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will tell you how many words are in your vocabulary with these vectors\n",
    "print(\"Number of word vectors available:\", len(word2vec_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOZwFyYu6Cmp",
    "outputId": "03129e87-1a17-4e5f-cbd5-fb0c6fa3a4fa"
   },
   "outputs": [],
   "source": [
    "def load_data(fname: str) -> list:\n",
    "    data = []\n",
    "    data_f = open(fname, 'r')\n",
    "    for line in data_f:\n",
    "        pieces = line.strip().split(\",\")\n",
    "        data.append((pieces[0], int(pieces[1])))\n",
    "    data_f.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "# Load in some training and testing data\n",
    "train = load_data(\"train_sentiment.txt\")\n",
    "dev = load_data(\"dev_sentiment.txt\")\n",
    "\n",
    "\n",
    "print(\"num in train:\", len(train))\n",
    "print(\"num in dev:\", len(dev))\n",
    "# what does the data look like?\n",
    "print(sorted(train)[:5])\n",
    "print(sorted(dev)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what is the overlap between the train and the dev sets?\n",
    "# how many words appear in both?\n",
    "# (hint: you'll need to extract just the word-parts of the train/dev sets to do this test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what is the distribution of labels in the train and dev sets?\n",
    "# how many positive and negative examples are there in each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TRMLxCw6Cmr",
    "outputId": "8078bb73-e12f-4b4e-83f1-94b91f50f102"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_input_array(examples: list, \n",
    "                       word_vectors: gensim.models.keyedvectors.Word2VecKeyedVectors) -> np.array:\n",
    "    \"\"\"\n",
    "    Convert examples to input and label arrays\n",
    "    Parameters:\n",
    "    examples (list): list of (word, label) tuples\n",
    "    word_vectors (gensim.models.keyedvectors.Word2VecKeyedVectors): dense word vectors to match with words\n",
    "\n",
    "    Returns:\n",
    "    numpy array of inputs, numpy array of corresponding labels, the words themselves (for debugging)\n",
    "    \"\"\"\n",
    "    # this will be our y\n",
    "    labels = []\n",
    "    \n",
    "    # this will be our input data\n",
    "    X = []\n",
    "    words = []\n",
    "    for ex in examples:\n",
    "        word = ex[0]\n",
    "        label = ex[1]\n",
    "        # ignore words that we don't have vectors for\n",
    "        if word in word_vectors:\n",
    "            # TODO: get the word vector that matches this target word \n",
    "            \n",
    "            \n",
    "            X.append(YOUR WORD VECTOR HERE)  \n",
    "            \n",
    "            labels.append(label)\n",
    "            \n",
    "            # collect the word too, just so that we can debug/investigate better\n",
    "            words.append(word)\n",
    "            \n",
    "    # TODO: make y into the correct shape (see below)\n",
    "    y = np.array([labels])\n",
    "    return np.array(X), y, words\n",
    "\n",
    "X, y, words = create_input_array(train, word2vec_vectors)\n",
    "\n",
    "# goal:\n",
    "# shape of inputs: (3342, 50)\n",
    "# Example embedding: [ 0.14702  -0.79382  -0.15014    ...\n",
    "# shape of labels: (3342, 1)\n",
    "# Example label: [1]\n",
    "# Example word: fresher\n",
    "\n",
    "print(\"shape of inputs:\", X.shape)\n",
    "print(\"Example embedding:\", X[0])\n",
    "print(\"shape of labels:\", y.shape)\n",
    "print(\"Example label:\", y[0])\n",
    "print(\"Example word:\", words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIG2zZXY6Cmv"
   },
   "source": [
    "Task 2: Using NN libraries\n",
    "----------------\n",
    "\n",
    "Now, we'll take a look at some common libraries used to create classifiers using neural nets. We'll take a look at [`keras`](https://keras.io/) which provides a nice API for implementing neural nets and can be run on top of TensorFlow, CNTK, or Theano. We'll look at an example using [`tensorflow`](https://github.com/tensorflow/tensorflow) as our backend.\n",
    "\n",
    "Installation of component libraries (NOTE: this is different if you are on a mac w/ an M1 or M2 chip! [link to Apple developer page](https://developer.apple.com/metal/tensorflow-plugin/) . Follow the instructions for \"Apple silicon\".):\n",
    "\n",
    "```\n",
    "pip3 install tensorflow\n",
    "sudo pip3 install keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4ipVZQo6Cmv"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mQNmkxi6Cmx",
    "outputId": "15d8cb69-2ff0-46cb-a7e3-03cee9876518"
   },
   "outputs": [],
   "source": [
    "hidden_units = # YOUR NUMBERS HERE\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "# set up the basis for a feed forward network\n",
    "model = Sequential()\n",
    "# hidden layer -- same number of hidden units as above\n",
    "model.add(Dense(units=hidden_units, activation='relu', input_dim=X.shape[1]))\n",
    "# output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# configure the learning process\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X, y, epochs=num_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is one single input to this network? __YOUR ANSWER HERE__\n",
    "2. What is one single output of this network? __YOUR ANSWER HERE__\n",
    "3. Test training your network with a few different numbers of hidden units. Try with a number __much smaller__ than the number of dimensions in your word vectors, a number __about the same__ as in your word vectors, and a number that is __much larger__. What numbers did you use and what are the final accuracies reported by your model? \n",
    "    1. much smaller: __YOUR NUM HIDDEN HERE__, __YOUR ACCURACY HERE__\n",
    "    2. about the same: __YOUR NUM HIDDEN HERE__, __YOUR ACCURACY HERE__\n",
    "    3. much larger: __YOUR NUM HIDDEN HERE__, __YOUR ACCURACY HERE__\n",
    "4. What loss functions are available for these models? [documentation](https://keras.io/api/models/model_training_apis/) __YOUR ANSWER HERE__\n",
    "5. Why is the loss function here `binary_crossentropy`? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-NR-F_Yo6Cmx",
    "outputId": "3befd517-ee3f-45db-8726-f3566072deab"
   },
   "outputs": [],
   "source": [
    "# evaluate on our development/test data\n",
    "# TODO: Get inputs and outputs for your dev set using code we've already implemented\n",
    "\n",
    "\n",
    "# if you just want labels\n",
    "# y_hat = model.predict(X_dev)\n",
    "\n",
    "# if you want score/accuracy\n",
    "score, acc = model.evaluate(X_dev, y_dev)\n",
    "\n",
    "# score is an evaluation of the loss function\n",
    "print(\"Score on dev:\", score)\n",
    "# Accuracy is \"standard accuracy\" of the model on this dev set\n",
    "print(\"Accuracy on dev:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How does your model perform on dev vs. train sets? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7V3p6sUX6Cmy",
    "outputId": "a1f3beb5-47de-43e9-8a3d-c73861dfbfc4"
   },
   "outputs": [],
   "source": [
    "# Finally, pick out some words that you'd like to test to see what\n",
    "# labels they are assigned.\n",
    "\n",
    "# come up with 3 - 5 test words of your own\n",
    "# these words must have word embeddings but need not have appeared in the train set\n",
    "test_words = # FILL IN HERE, you want a list of tuples, like train and dev\n",
    "\n",
    "# TODO: make into corresponding X and y\n",
    "\n",
    "\n",
    "# TODO: make some predictions\n",
    "\n",
    "\n",
    "# TODO: make sure that you can see which predictions were correct vs. incorrect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: playing with `yield`\n",
    "-------\n",
    "\n",
    "In your final HW, you will write a function that uses the python keyword `yield` to produce batches of input data for your models that you're training. Play around with the functions below to understand more how this works.\n",
    "\n",
    "[documentation for `yield`](https://docs.python.org/3/reference/simple_stmts.html#the-yield-statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_squares(n):\n",
    "    for i in range(n):\n",
    "        yield i*i \n",
    "    \n",
    "    \n",
    "# TODO: play around with the below code\n",
    "# add more! \n",
    "# how would you see the number 16?\n",
    "# what happens after 16?\n",
    "result = generate_squares(5)\n",
    "print(next(result))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
