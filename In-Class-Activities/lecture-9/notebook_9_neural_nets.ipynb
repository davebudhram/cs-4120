{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 9: Intro to Neural Nets\n",
    "===============\n",
    "\n",
    "10/12/2023, CS 4/6120 Natural Language Processing, Muzny\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Writing a neural net from scratch\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# seed random number generation so that you can \n",
    "# track the same numbers as each other\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Apply the sigmoid function (1 / (1 + e^(-x)))\n",
    "    to the passed in value.\n",
    "    Parameters:\n",
    "        x - float value to pass through sigmoid\n",
    "    Return:\n",
    "    float in [0, 1]\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Apply the derivative of the sigmoid function\n",
    "    sigmoid(x) * (1 - sigmoid(x))\n",
    "    to the passed in value.\n",
    "    Parameters:\n",
    "        x - float value to pass through sigmoid derivative\n",
    "    Return:\n",
    "    float result\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dataset\n",
    "# 3rd \"feature\" is the bias term\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# labels, transposed so that they match\n",
    "# easily with our inputs X\n",
    "# the first label matches the first row in our input data,\n",
    "# the second label matches the second row in our input data, etc\n",
    "# .T gets the transpose for us, which makes \n",
    "# matrix math easier later\n",
    "y = np.array([[0,1,1,0]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What logical function (AND, OR, etc) does this dataset represent? (remember that this function should apply to two inputs (our two input features and produce the matching label)\n",
    "\n",
    "__YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[-0.39151551  0.04951286 -0.13610996 -0.41754172]\n",
      " [ 0.22370579 -0.72101228 -0.4157107  -0.26727631]\n",
      " [-0.08786003  0.57035192 -0.60065244  0.02846888]]\n",
      "U: [[ 0.18482914]\n",
      " [-0.90709917]\n",
      " [ 0.2150897 ]\n",
      " [-0.65895175]]\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 4\n",
    "input_features = X.shape[1]\n",
    "\n",
    "# initialize weights randomly with mean 0 and range [-1, 1]\n",
    "# TODO: fill in dimensions here for W and U\n",
    "# fill these in as a tuple like (rows, columns)\n",
    "# this corresponds to how shapes are represented for numpy arrays\n",
    "W_dim = (input_features, hidden_units)\n",
    "\n",
    "# you'll need to use W_dim and U_dim to produce the\n",
    "# correct number of random numbers\n",
    "W = 2 * np.random.random(W_dim) - 1\n",
    "# note that we are doing binary classification, so the second dimension here is 1 \n",
    "# (corresponding to one output unit)\n",
    "U_dim = (hidden_units, 1)\n",
    "U = 2 * np.random.random(U_dim) - 1\n",
    "print(\"W:\", W)\n",
    "print(\"U:\", U)\n",
    "\n",
    "\n",
    "inputs = X\n",
    "num_epochs = 1000\n",
    "for i in range(num_epochs):\n",
    "    # forward propagation—sigmoid, relu, tanh, etc\n",
    "    h = sigmoid(np.dot(inputs,W))\n",
    "    \n",
    "    # always sigmoid—classification\n",
    "    # note that this gives us the classification for every input\n",
    "    # example simultaneously\n",
    "    y_hat = sigmoid(np.dot(h,U))\n",
    "\n",
    "    # how much did we miss?\n",
    "    layer2_error = y - y_hat\n",
    "    \n",
    "    # this is telling us how much to move\n",
    "    # our weights and in what direction\n",
    "    # use the corresponding derivative to the non-linearity used above\n",
    "    layer2_delta = layer2_error * sigmoid_deriv(y_hat)\n",
    "\n",
    "    # how much did each L1 value contribute to \n",
    "    # the L2 error (according to the weights)?\n",
    "    layer1_error = layer2_delta.dot(U.T)\n",
    "    \n",
    "    # this is telling us how much to move\n",
    "    # our weights and in what direction\n",
    "    layer1_delta = layer1_error * sigmoid_deriv(h)\n",
    "\n",
    "    U += h.T.dot(layer2_delta)\n",
    "    W += inputs.T.dot(layer1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Does the hidden layer have a bias term in this neural net? __YOUR ANSWER HERE__\n",
    "3. What variables' values are updated as the loop above iterates? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output After Training:\")\n",
    "# these are the same as the inputs that we trained this net on\n",
    "test_inputs = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "gold_labels = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# TODO: Write the code to assign labels to the test data\n",
    "h = FILL ME IN\n",
    "y_hat = FILL ME IN\n",
    "\n",
    "# These should match with each other\n",
    "# y was our gold labels from the beginning\n",
    "print(\"Actual labels:\", gold_labels.T)\n",
    "print(\"Assigned probabilities:\", y_hat)\n",
    "print(\"Assigned labels:\", [1 if y_hat_val > .5 else 0 for y_hat_val in y_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many iterations did you need for the predicted values $\\hat y$ to match the actual values? __YOUR ANSWER HERE__\n",
    "5. Make a graph of how the `layer2_error` changes as epochs progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Neural Nets from libraries (you'll be doing a similar thing in your sentiment analysis HW!)\n",
    "----------------\n",
    "\n",
    "Now, we'll take a look at some common libraries used to create classifiers using neural nets. We'll take a look at [`keras`](https://keras.io/) which provides a nice API for implementing neural nets and can be run on top of TensorFlow, CNTK, or Theano. We'll look at an example using [`tensorflow`](https://github.com/tensorflow/tensorflow) as our backend.\n",
    "\n",
    "Installation of component libraries:\n",
    "\n",
    "```\n",
    "pip3 install tensorflow\n",
    "sudo pip3 install keras\n",
    "```\n",
    "\n",
    "If you are working on a Silicon chip Mac (Macs with M1 and M2 chips), you'll need at least OS 12.0+ (Monterey (12) or Ventura (13)), then you'll want to follow the [instructions on the Apple developers website](https://developer.apple.com/metal/tensorflow-plugin/). We will be using tensorflow/keras going forward, so this is worth doing on your own outside of class!\n",
    "\n",
    "In the meantime, you can also upload this notebook to [Google colaboratory](https://colab.research.google.com/) and run this portion on the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines of code to do the import. Left\n",
    "# commented because on Macs with unsupported architecture, these\n",
    "# imports will kill your kernel which is highly annoying.\n",
    "\n",
    "\n",
    "# Sequential will be the base model we'll use\n",
    "from keras.models import Sequential\n",
    "# Dense layers are our base feed forward layers\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the basis for a feed forward network\n",
    "model = Sequential()\n",
    "\n",
    "# same X and y as above\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# hidden layer\n",
    "# you can play around with different activation functions\n",
    "model.add(Dense(units=4, activation='relu', input_dim=X.shape[1]))\n",
    "\n",
    "# output layer\n",
    "# activation function is our classification function\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# configure the learning process\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 1 epoch = once through the data\n",
    "model.fit(X, y, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "y_test = np.array([[0,1,1,0]]).T\n",
    "labels = model.predict(x_test)\n",
    "print(\"Assigned probabilities:\", labels)\n",
    "print(\"Assigned labels:\", [1 if y_hat_val > .5 else 0 for y_hat_val in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How many epochs did you need for 100% accuracy? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested in getting deeper into neural nets? \n",
    "\n",
    "\n",
    "Here are two places to start from:\n",
    "- take a look at the data that you can load from [`nltk`](https://www.nltk.org/data.html) and [`scikit-learn`](https://scikit-learn.org/stable/datasets/index.html#dataset-loading-utilities), then work on creating a neural net to do either binary or multinomial classification\n",
    "- take a look at the tensorflow + keras word embeddings tutorial [here](https://www.tensorflow.org/tutorials/text/word_embeddings). Note! This tutorial mentions RNNs, which are special kind of neural net (they are not the feedforward architecture that we've seen so far). We'll get into RNNs after next week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
