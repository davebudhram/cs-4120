{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 8: Logistic Regression Examples\n",
    "===============\n",
    "\n",
    "10/5/2023, CS 4/6120 Natural Language Processing, Muzny\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Logistic Regression\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # helpful for dot products\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement and comment the sigmoid function!\n",
    "# access the value of e either via math.e or np.e\n",
    "# sigmoid(z) = 1 / (1 + e^-z)\n",
    "\n",
    "def sigmoid(z: float) -> float:\n",
    "    return 1 / (1 + math.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x, w, b\n",
    "x = [3, 2, 1, 3, 0, 4.15]\n",
    "w = [2.5, -5, -1.2, 0.5, 2, 0.7]\n",
    "b = 0.1  # initialize to some value\n",
    "y = 1  # true label of this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-score : 0.8049999999999998\n",
      "p1 : 0.6910430124157227\n",
      "p0 : 0.30895698758427725\n",
      "Gradients : [-0.9268709627528318, -0.6179139751685545, -0.30895698758427725, -0.9268709627528318, -0.0, -1.2821714984747508]\n",
      "Gradient Changes\n",
      "weights : [3.4268709627528318, -4.3820860248314455, -0.8910430124157227, 1.4268709627528318, 2.0, 1.9821714984747507]\n",
      "z-score : 13.232022433108593\n",
      "p1 : 0.9999982077239783\n",
      "p0 : 1.792276021705952e-06\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement one update from SGD\n",
    "# print the intermediate values as you go\n",
    "\n",
    "# 1. calculate P(y = 1) and P(y = 0), print out their values\n",
    "z = np.dot(x, w) + b \n",
    "print(f\"z-score : {z}\")\n",
    "p_1 = sigmoid(z)\n",
    "p_0 = 1 - p_1\n",
    "print(f\"p1 : {p_1}\")\n",
    "print(f\"p0 : {p_0}\")\n",
    "\n",
    "# 2. calculate your gradients for each weight, print them out\n",
    "gradients = []\n",
    "for feature in x:\n",
    "  gradients.append((p_1-1) * feature)\n",
    "print(f\"Gradients : {gradients}\")\n",
    "\n",
    "# 3. calculate your updated weights, print them out\n",
    "learning_rate = 1\n",
    "\n",
    "new_weights = []\n",
    "for weight, gradient in zip(w, gradients):\n",
    "  new_weights.append(weight - learning_rate*gradient)\n",
    "print(\"Gradient Changes\")\n",
    "print(f\"weights : {new_weights}\")\n",
    "# 4. Check how you did! Calculate P(y = 1) and P(y = 0) again\n",
    "z = np.dot(x, new_weights) + b \n",
    "print(f\"z-score : {z}\")\n",
    "new_p_1 = sigmoid(z)\n",
    "new_p_0 = 1 - new_p_1\n",
    "print(f\"p1 : {new_p_1}\")\n",
    "print(f\"p0 : {new_p_0}\")\n",
    "# 5. test out the effects of changing the value of your learning rate \n",
    "# What about also updating your bias term?\n",
    "\n",
    "\n",
    "# YOUR ANSWERS HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
