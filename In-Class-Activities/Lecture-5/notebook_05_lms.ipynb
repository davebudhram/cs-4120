{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 5: Language Models (part 2)\n",
    "===============\n",
    "\n",
    "9/21/2023, CS 4/6120 Natural Language Processing, Muzny\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: N-grams\n",
    "----------\n",
    "\n",
    "Make sure that you have one or two text files (e.g. `mobydick.txt`, `alice_begin.txt`, `audrelorde.txt`) in an accessible place for this part of the exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is an n-gram:\n",
    "# windows of words of n length\n",
    "# I like Boston because it is nice\n",
    "# n = 2\n",
    "# I, like\n",
    "# like, Boston,\n",
    "# Boston, because\n",
    "# number of 2-grams: 7 - 2 + 1 = 6\n",
    "\n",
    "# n = 3\n",
    "# I, like, Boston\n",
    "# like, Boston, because\n",
    "# Boston, because, it\n",
    "# number of 3-grams: 7 - 3 + 1 = 5\n",
    "\n",
    "# Number of n-grams for a sequence of tokens of length t:\n",
    "# t - n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davebudhram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# we'll use nltk for tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# make any graphs appear inline in this notbook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename: str) -> list:\n",
    "    \"\"\"Loads tokens using nltk's word_tokenize for the given file.\n",
    "    Args:\n",
    "    filename (str): path to file\n",
    "\n",
    "    Returns:\n",
    "    list: list of tokens\n",
    "    \"\"\"\n",
    "    # write a filepath here to whatever text file you'd like!\n",
    "    f = open(filename, \"r\", encoding=\"utf-8\")\n",
    "    content = f.read()  # read all contents\n",
    "    f.close()  # close the file when you're done\n",
    "\n",
    "    # print out the number of tokens that you get from word_tokenize\n",
    "    print(\"Total length (characters):\", len(content))\n",
    "\n",
    "    # if you don't have nltk installed, fine to\n",
    "    # replace this with content.split()\n",
    "    toks = nltk.word_tokenize(content)\n",
    "    print(\"Tokens:\", len(toks))\n",
    "    \n",
    "    # TODO: update this line of code\n",
    "    vocab_size = len(set(toks))\n",
    "    \n",
    "    print(\"Vocabulary:\", vocab_size) \n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(tokens: list, n: int) -> list:\n",
    "    \"\"\"Creates n-grams for the given token sequence.\n",
    "    Args:\n",
    "    tokens (list): a list of tokens as strings\n",
    "    n (int): the length of n-grams to create\n",
    "\n",
    "    Returns:\n",
    "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "    \n",
    "    if n = 2, then:\n",
    "    [(token1, token2), (token2, token3), ....]\n",
    "    \"\"\"\n",
    "    # TODO: implement this function!\n",
    "    # hint: you'll need this function for your next homework :)\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[j] for j in range(i, i + n))\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length (characters): 593\n",
      "Tokens: 128\n",
      "Vocabulary: 83\n",
      "[('Alice', 'was', 'beginning'), ('was', 'beginning', 'to'), ('beginning', 'to', 'get'), ('to', 'get', 'very'), ('get', 'very', 'tired'), ('very', 'tired', 'of'), ('tired', 'of', 'sitting'), ('of', 'sitting', 'by'), ('sitting', 'by', 'her'), ('by', 'her', 'sister'), ('her', 'sister', 'on'), ('sister', 'on', 'the'), ('on', 'the', 'bank'), ('the', 'bank', ','), ('bank', ',', 'and'), (',', 'and', 'of'), ('and', 'of', 'having'), ('of', 'having', 'nothing'), ('having', 'nothing', 'to'), ('nothing', 'to', 'do'), ('to', 'do', ':'), ('do', ':', 'once'), (':', 'once', 'or'), ('once', 'or', 'twice'), ('or', 'twice', 'she'), ('twice', 'she', 'had'), ('she', 'had', 'peeped'), ('had', 'peeped', 'into'), ('peeped', 'into', 'the'), ('into', 'the', 'book'), ('the', 'book', 'her'), ('book', 'her', 'sister'), ('her', 'sister', 'was'), ('sister', 'was', 'reading'), ('was', 'reading', ','), ('reading', ',', 'but'), (',', 'but', 'it'), ('but', 'it', 'had'), ('it', 'had', 'no'), ('had', 'no', 'pictures'), ('no', 'pictures', 'or'), ('pictures', 'or', 'conversations'), ('or', 'conversations', 'in'), ('conversations', 'in', 'it'), ('in', 'it', ','), ('it', ',', '“'), (',', '“', 'and'), ('“', 'and', 'what'), ('and', 'what', 'is'), ('what', 'is', 'the'), ('is', 'the', 'use'), ('the', 'use', 'of'), ('use', 'of', 'a'), ('of', 'a', 'book'), ('a', 'book', ','), ('book', ',', '”'), (',', '”', 'thought'), ('”', 'thought', 'Alice'), ('thought', 'Alice', '“'), ('Alice', '“', 'without'), ('“', 'without', 'pictures'), ('without', 'pictures', 'or'), ('pictures', 'or', 'conversations'), ('or', 'conversations', '?'), ('conversations', '?', '”'), ('?', '”', 'So'), ('”', 'So', 'she'), ('So', 'she', 'was'), ('she', 'was', 'considering'), ('was', 'considering', 'in'), ('considering', 'in', 'her'), ('in', 'her', 'own'), ('her', 'own', 'mind'), ('own', 'mind', '('), ('mind', '(', 'as'), ('(', 'as', 'well'), ('as', 'well', 'as'), ('well', 'as', 'she'), ('as', 'she', 'could'), ('she', 'could', ','), ('could', ',', 'for'), (',', 'for', 'the'), ('for', 'the', 'hot'), ('the', 'hot', 'day'), ('hot', 'day', 'made'), ('day', 'made', 'her'), ('made', 'her', 'feel'), ('her', 'feel', 'very'), ('feel', 'very', 'sleepy'), ('very', 'sleepy', 'and'), ('sleepy', 'and', 'stupid'), ('and', 'stupid', ')'), ('stupid', ')', ','), (')', ',', 'whether'), (',', 'whether', 'the'), ('whether', 'the', 'pleasure'), ('the', 'pleasure', 'of'), ('pleasure', 'of', 'making'), ('of', 'making', 'a'), ('making', 'a', 'daisy-chain'), ('a', 'daisy-chain', 'would'), ('daisy-chain', 'would', 'be'), ('would', 'be', 'worth'), ('be', 'worth', 'the'), ('worth', 'the', 'trouble'), ('the', 'trouble', 'of'), ('trouble', 'of', 'getting'), ('of', 'getting', 'up'), ('getting', 'up', 'and'), ('up', 'and', 'picking'), ('and', 'picking', 'the'), ('picking', 'the', 'daisies'), ('the', 'daisies', ','), ('daisies', ',', 'when'), (',', 'when', 'suddenly'), ('when', 'suddenly', 'a'), ('suddenly', 'a', 'White'), ('a', 'White', 'Rabbit'), ('White', 'Rabbit', 'with'), ('Rabbit', 'with', 'pink'), ('with', 'pink', 'eyes'), ('pink', 'eyes', 'ran'), ('eyes', 'ran', 'close'), ('ran', 'close', 'by'), ('close', 'by', 'her'), ('by', 'her', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Test your `load_tokens` and `make_ngrams` functions\n",
    "# make sure to test different values of n\n",
    "# we recommend using the Audre Lorde or Alice in Wonderland text to\n",
    "# start off (test on a small text, then run on larger ones!)\n",
    "tokens = load_tokens(\"./alice_begin.txt\")\n",
    "n = 3\n",
    "# if you have more than ~50 tokens, we recommend only looking at the first ~ 50.\n",
    "# this is a good time to learn about list slicing in python if you haven't used it before.\n",
    "# this selects a sublist from a list with the syntax:\n",
    "#\n",
    "# list[begin index inclusive:end index exclusive]\n",
    "print(make_ngrams(tokens, n))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
