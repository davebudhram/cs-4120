{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 4: vector similarity\n",
    "===============\n",
    "\n",
    "9/18/2023, CS 4/6120 Natural Language Processing, Muzny\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Calculating tf-idf\n",
    "----\n",
    "\n",
    "To calculate tf-idf, we'll need to first construct a term-document matrix from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davebudhram/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# for tokenization, not necessary\n",
    "# (comment out if you don't have nltk installed yet)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# helpful for calculating log10 and because\n",
    "# numpy arrays make certain manipulations\n",
    "# (like getting a column of numbers)\n",
    "# easier\n",
    "import numpy as np\n",
    "\n",
    "# useful for counting :)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term: str, document: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate term frequency\n",
    "    Parameters:\n",
    "    term - string\n",
    "    document - list of strings (tokenized document)\n",
    "    Return:\n",
    "    float term frequency\n",
    "    \"\"\"\n",
    "    return np.log10(document.count(term) + 1)\n",
    "\n",
    "def idf(N: int, df_t: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate inverse document frequency given the \n",
    "    number of documents and the number of documents the term appears in.\n",
    "    Paramenters:\n",
    "    N - int (number of documents)\n",
    "    df_t - int (number of documents the term appears in)\n",
    "    Return:\n",
    "    float inverse document frequency\n",
    "    \"\"\"\n",
    "    return np.log10(N / df_t)\n",
    "\n",
    "\n",
    "def term_in_documents(term: str, documents: list) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of documents in a list of documents that a target\n",
    "    term appears in.\n",
    "    Parameters:\n",
    "    term - str\n",
    "    documents - list of list of str (list of tokenized documents)\n",
    "    Return:\n",
    "    int number of documents the term appears in\n",
    "    \"\"\"\n",
    "    total = 0 \n",
    "    for document in documents:\n",
    "        if term in document:\n",
    "            total +=1\n",
    "    return total\n",
    "\n",
    "\n",
    "# load in the data\n",
    "def load_tokens(filename):\n",
    "    f = open(filename, 'r')\n",
    "    contents = f.read().lower()\n",
    "    f.close()\n",
    "    # if you don't have nltk installed, use another tokenization\n",
    "    # strategy here like str.split()\n",
    "    return nltk.word_tokenize(contents)\n",
    "\n",
    "mobydick = load_tokens('./mobydick.txt')\n",
    "shakes = load_tokens('./shakesdown.txt')\n",
    "pandp = load_tokens('./prideandprejudice.txt')\n",
    "books = [mobydick, shakes, pandp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we won't create the entire term-document matrix, we'll just do it for a few key terms that we\n",
    "# care about for the sake of time\n",
    "# TODO: pick 3 - 5 words\n",
    "words = ['the', 'girl', 'people']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the term-document matrix\n",
    "matrix = []\n",
    "\n",
    "# iterate through your chosen words\n",
    "for word in words:\n",
    "    # count how many times it occurs in each book\n",
    "    # this is just for debugging for you\n",
    "    counts = [str(b.count(word)) for b in books]\n",
    "    print(word, \":\\t\", \"\\t\".join(counts))\n",
    "    \n",
    "    \n",
    "    # calculate the term frequency for each book\n",
    "    # for this word\n",
    "    # tf_words should be the same length as counts\n",
    "    # tf_words = YOUR CODE HERE\n",
    "    tf_words = []\n",
    "    for book in books:\n",
    "       tf_words.append(tf(word, book))\n",
    "    \n",
    "    # calculate idf for this term\n",
    "    # this will be a single scalar\n",
    "    N = len(books)\n",
    "    df_t = term_in_documents(word, books)\n",
    "    idf_t = idf(N, df_t)\n",
    "    \n",
    "    # multiply tf with idf for each book/each\n",
    "    # term frequency you calculated\n",
    "    tfidf_words = \n",
    "    \n",
    "    # add the tfidf numbers to your matrix\n",
    "    matrix.append(tfidf_words)\n",
    "    \n",
    "    # uncomment to see visually the different components (helpful for debugging)\n",
    "#     print(word, \" tf:\\t\", \"\\t\".join([str(x) for x in tf_words]))\n",
    "#     print(word, \"idf:\\t\", idf_t)\n",
    "#     print(word, \" tf-idf:\\t\", \"\\t\".join([str(x) for x in tfidf_words]))\n",
    "    \n",
    "# if you'd like to, uncomment the following code to make the matrix into a numpy array\n",
    "# matrix = np.array(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What should the dimensions of your matrix be? __number of words (5) by number of books (3)__\n",
    "2. What happens if you attempt to calculate tfidf of a term that exists in *none* of your books? __Error, unless you adjust the code__\n",
    "2. What happens if you attempt to calculate tfidf of a term that exists in *all* of your books? __it's zero!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimensions of your matrix\n",
    "# number of rows should match number of words\n",
    "# number of cols should match number of documents (books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(v1: list, v2: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors of the same length.\n",
    "    Parameters:\n",
    "    v1 - list (of numbers)\n",
    "    v2 - list (of numbers)\n",
    "    Return:\n",
    "    float cosine similarity\n",
    "    \"\"\"\n",
    "    # you may find the numpy functions np.dot() and np.linalg.norm() useful\n",
    "    # TODO: implement\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# calculate the similarity between two *word* vectors\n",
    "# we'll just do word vectors because unless matrix is a numpy array\n",
    "# it is (more) difficult to get column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you finish, which book is closest to moby dick?\n",
    "# you'll need a *column* vector here (instead of a row vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you finish finish, which book is closest to moby dick, but remake your matrix with all vocabulary terms?\n",
    "# create the term-document matrix\n",
    "# you'll want to use counters for each book's vocabulary for the sake of efficiency\n",
    "\n",
    "\n",
    "# you may want to re-define new counter versions of your tf, idf, term_in_documents \n",
    "# functions so that they work with counters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the shape of your matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which book is actually closest to moby dick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: install `nltk` (this is the same task 2 from lecture 3)\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you finish the first task, work on making sure that you have `nltk` downloaded and accessible to your jupyter notebooks. While you will not be allowed to use `nltk` for *most* of your homework, we will use it frequently in class to demonstrate tools. \n",
    "\n",
    "[`nltk`](https://www.nltk.org/) (natural language toolkit) is a python package that comes with many useful implementations of NLP tools and datasets.\n",
    "\n",
    "From the command line, using pip: `pip3 install nltk`\n",
    "\n",
    "[installing nltk](https://www.nltk.org/install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# the stemmer we'll use\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# also grab a lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# for the tokenizer that we're going to use\n",
    "# won't cause an error if you've already downloaded it\n",
    "nltk.download('punkt')\n",
    "# for the lemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"N.K. Jemison is a science fiction author.\"\n",
    "words = nltk.word_tokenize(example)\n",
    "\n",
    "# not perfect, but much better\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the nltk tokenizer, tokenize Moby Dick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many tokens do you have now? how big is the vocabulary? Are these numbers larger or smaller than using `str.split()` to tokenize? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# example stemming at the individual token level\n",
    "for w in moby_nltk_tokens[:5]:\n",
    "    print(porter_stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How big is the vocabulary of stems? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many lemmas in the vocabulary?\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "# the lemmatizer works using the method .lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How big is the vocabulary of lemmas? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. make a graph of heap's law with separate series for tokens, stems, and lemmas. Do they follow the same patterns? __YOUR ANSWER HERE__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
