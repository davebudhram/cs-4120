{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Feedforward Neural Language Model (60 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils as nutils\n",
    "\n",
    "# Importing utility functions from Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# necessary\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# optional\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm import notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# your other imports here\n",
    "import time\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3 # The ngram language model you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/davebudhram/Documents/Northeastern/CS4120/cs-4120/Homework-5/neurallm_task3.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/cs-4120/Homework-5/neurallm_task3.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m EMBEDDING_SAVE_FILE_CHAR \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mspooky_embedding_char.txt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# The file to save your word embeddings to\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/cs-4120/Homework-5/neurallm_task3.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m TRAIN_FILE \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mspooky_author_train.csv\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# The file to train your language model on\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/cs-4120/Homework-5/neurallm_task3.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data_word \u001b[39m=\u001b[39m nutils\u001b[39m.\u001b[39mread_file_spooky(TRAIN_FILE, ngram\u001b[39m=\u001b[39mNGRAM)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davebudhram/Documents/Northeastern/CS4120/cs-4120/Homework-5/neurallm_task3.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m data_char \u001b[39m=\u001b[39m nutils\u001b[39m.\u001b[39mread_file_spooky(TRAIN_FILE, ngram\u001b[39m=\u001b[39mNGRAM, by_character\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nutils' is not defined"
     ]
    }
   ],
   "source": [
    "# load in necessary data\n",
    "EMBEDDING_SAVE_FILE_WORD = \"spooky_embedding_word.txt\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = \"spooky_embedding_char.txt\" # The file to save your word embeddings to\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "data_word = nutils.read_file_spooky(TRAIN_FILE, ngram=NGRAM)\n",
    "data_char = nutils.read_file_spooky(TRAIN_FILE, ngram=NGRAM, by_character=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Tokenizer and fit on your data\n",
    "# do this for both the word and character data\n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "tokenizer_char = Tokenizer(char_level=True)\n",
    "tokenizer_char.fit_on_texts(data_char)\n",
    "encoded_char = tokenizer_char.texts_to_sequences(data_char)\n",
    "\n",
    "tokenizer_word = Tokenizer(char_level=False)\n",
    "tokenizer_word.fit_on_texts(data_word)\n",
    "encoded_word = tokenizer_word.texts_to_sequences(data_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "25374\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the word index for each of your tokenizers\n",
    "# this should match what you calculated in Task 2 with your embeddings\n",
    "\n",
    "char_word_index = tokenizer_char.word_index\n",
    "print(len(char_word_index))\n",
    "word_word_index = tokenizer_word.word_index\n",
    "print(len(word_word_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depending on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process                                    however\n",
    "    process, however                                    afforded\n",
    "    however, afforded\t                                me\n",
    "\n",
    "\n",
    "Our first step is to translate the text into sequences of numbers, \n",
    "one sequence per n-gram window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Training Samples\n",
      "Count: 634080\n",
      "[1, 1, 32]\n",
      "[1, 32, 2956]\n",
      "[32, 2956, 3]\n",
      "[2956, 3, 155]\n",
      "[3, 155, 3]\n",
      "\n",
      "Char Training Samples\n",
      "Count: 2957553\n",
      "[21, 21, 3]\n",
      "[21, 3, 9]\n",
      "[3, 9, 7]\n",
      "[9, 7, 8]\n",
      "[7, 8, 1]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram_training_samples(encoded: list, ngram: int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    training_samples = []\n",
    "\n",
    "    for encoded_list in encoded:\n",
    "        # encoded_list does not have enough elements to create a single ngram\n",
    "        if len(encoded_list) < ngram:\n",
    "            continue \n",
    "\n",
    "        # We will end up with (#elements - ngram + 1) lists. Ex. a list with two elements and ngram size two would only produce one list \n",
    "        for i in range(len(encoded_list) - ngram + 1):\n",
    "            ngram_sample = encoded_list[i: i+ngram] # If ngram = 3 get [0:4], [1:5], ...\n",
    "            # x = ngram_sample[:-1]  # All elements except the last one\n",
    "            # y = ngram_sample[-1]   # The last element\n",
    "            # training_samples.append(x + [y])\n",
    "            training_samples.append(ngram_sample)\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [21, 21, 3]\n",
    "# [21, 3, 9]\n",
    "# [3, 9, 7]\n",
    "# ...\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [1, 1, 32]\n",
    "# [1, 32, 2956]\n",
    "# [32, 2956, 3]\n",
    "# ...\n",
    "\n",
    "training_samples_word = generate_ngram_training_samples(encoded_word, ngram=NGRAM)\n",
    "print('Word Training Samples')\n",
    "print('Count:', len(training_samples_word))\n",
    "for i in range(5):\n",
    "    print(training_samples_word[i])\n",
    "training_samples_char = generate_ngram_training_samples(encoded_char, ngram=NGRAM)\n",
    "print('\\nChar Training Samples')\n",
    "print('Count:', len(training_samples_char))\n",
    "for i in range(5):\n",
    "    print(training_samples_char[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634080\n",
      "2\n",
      "634080\n",
      "2957553\n",
      "2\n",
      "2957553\n",
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here\n",
    "def splitSequence(samples):\n",
    "  x_list = [] # list of list of x values\n",
    "  y_list = [] # list of y values\n",
    "  for sequence in samples:\n",
    "    x_values = sequence[:-1] # Everything but the last element\n",
    "    y_value = sequence[-1] # Last element\n",
    "    x_list.append(x_values)\n",
    "    y_list.append(y_value)\n",
    "  return x_list, y_list\n",
    "\n",
    "X_word, y_word = splitSequence(training_samples_word)\n",
    "X_char, y_char = splitSequence(training_samples_char)\n",
    "\n",
    "# print out the shapes to verify that they are correct\n",
    "print(len(X_word))\n",
    "print(len(X_word[0]))\n",
    "print(len(y_word))\n",
    "print(len(X_char))\n",
    "print(len(X_char[0]))\n",
    "print(len(y_char))\n",
    "\n",
    "print(X_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Initialize a function that reads the word embeddings you saved earlier\n",
    "# and gives you back mappings from words to their embeddings and also \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def read_embeddings(filename: str, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters:\n",
    "        filename (str): path to file\n",
    "        Tokenizer: tokenizer used to tokenize the data (needed to get the word to index mapping)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    word_to_embedding = {}  # Mapping from word to its embedding vector\n",
    "    index_to_embedding = {}  # Mapping from index to its embedding vector\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            split_line = line.split()\n",
    "            # Skip the first line of file\n",
    "            if len(split_line) == 2:\n",
    "                continue\n",
    "            word = split_line[0]\n",
    "            vector = [float(x) for x in split_line[1:]]\n",
    "        \n",
    "            if word in tokenizer.word_index:\n",
    "                word_to_embedding[word] = vector # Mapping from word to its embedding vector\n",
    "                index_to_embedding[tokenizer.word_index[word]] = vector # Mapping from index to its embedding vector\n",
    "    return word_to_embedding, index_to_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# NECESSARY FOR CHARACTERS\n",
    "\n",
    "# the \"0\" index of the Tokenizer is assigned for the padding token. Initialize\n",
    "# the vector for padding token as all zeros of embedding size\n",
    "# this adds one to the number of embeddings that were initially saved\n",
    "# (and increases your vocab size by 1)\n",
    "word_embedding_word, word_embedding_index = read_embeddings(EMBEDDING_SAVE_FILE_WORD, tokenizer_word)\n",
    "word_embedding_index[0] = [0] * len(word_embedding_index[1])\n",
    "char_embedding_word , char_embedding_index = read_embeddings(EMBEDDING_SAVE_FILE_CHAR, tokenizer_char)\n",
    "char_embedding_index[0] = [0] * len(char_embedding_index[1]) # Add a list of 0s for index \"0\" (got size from index \"1\" )\n",
    "print(char_embedding_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, index_2_embedding: dict, for_feedforward:bool=False) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    If for_feedforward is True: \n",
    "    Returns data generator to be used by feed_forward\n",
    "    else: Returns data generator for RNN model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    def toEmbeddings(x_vector):\n",
    "        result = []\n",
    "        for token in x_vector:\n",
    "            result.extend(index_2_embedding[token])\n",
    "        return result\n",
    "\n",
    "    num_samples = len(X)\n",
    "    i = 0\n",
    "    while i < num_samples:\n",
    "        # Apply padding when we our last batch won't have enough and we for_feedforward is false\n",
    "        if not for_feedforward and i + num_sequences_per_batch > num_samples: \n",
    "            print('here')\n",
    "            batch_X = X[i:]\n",
    "            batch_y = y[i:]\n",
    "            for j in range((i +  num_sequences_per_batch) - num_samples):\n",
    "                n_gram = len(batch_X[0])\n",
    "                batch_X.append([0] * n_gram)\n",
    "                batch_y.append(0)\n",
    "        else:\n",
    "            batch_X = X[i:i + num_sequences_per_batch]\n",
    "            batch_y = y[i:i + num_sequences_per_batch]\n",
    "        # Convert batch_X to embeddings using index_2_embedding \n",
    "        embeddings = [toEmbeddings(x_vector) for x_vector in batch_X]\n",
    "        # Encode batch_y as one-hot vectors of size len(index_2_embedding) \n",
    "        one_hot_vectors = to_categorical(batch_y, num_classes=len(index_2_embedding))\n",
    "        yield np.array(embeddings), one_hot_vectors\n",
    "        i += num_sequences_per_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(128, 25375)\n"
     ]
    }
   ],
   "source": [
    "data_gen = data_generator(X_word, y_word, 128, word_embedding_index, for_feedforward=True)\n",
    "i = 0\n",
    "for batch_X, batch_y in data_gen:\n",
    "  i += len(batch_X)\n",
    "  print(np.shape(batch_X[0]))\n",
    "  print(np.shape(batch_y))\n",
    "  if i == 128:\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 100)\n",
      "(128, 25375)\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# initialize your data_generator for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is correct for both word and character data\n",
    "\n",
    "# Examples:\n",
    "# num_sequences_per_batch = 128 # this is the batch size\n",
    "# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical\n",
    "\n",
    "num_sequences_per_batch = 128 # this is the batch size\n",
    "steps_per_epoch = len(X_word)//num_sequences_per_batch  # Number of batches per epoch\n",
    "train_generator = data_generator(X_word, y_word, num_sequences_per_batch, word_embedding_index)\n",
    "sample = next(train_generator)\n",
    "print(sample[0].shape)\n",
    "\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Train & __save__ your models (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_35 (Dense)            (None, 3)                 303       \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 25375)             101500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101803 (397.67 KB)\n",
      "Trainable params: 101803 (397.67 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 15 points \n",
    "\n",
    "# code to train a feedforward neural language model for \n",
    "# both word embeddings and character embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "# (define functions as needed)\n",
    "\n",
    "# train your models for between 3 & 5 epochs\n",
    "# on Felix's machine, this takes ~ 24 min for character embeddings and ~ 10 min for word embeddings\n",
    "# DO NOT EXPECT ACCURACIES OVER 0.5 (and even that is very for this many epochs)\n",
    "# We recommend starting by training for 1 epoch\n",
    "\n",
    "# Define your model architecture using Keras Sequential API\n",
    "# Use the adam optimizer instead of sgd\n",
    "# add cells as desired\n",
    "\n",
    "model_word = Sequential()\n",
    "\n",
    "# hidden layer\n",
    "# you can play around with different activation functions\n",
    "model_word.add(Dense(units=3, activation='relu', input_dim=100))\n",
    "\n",
    "\n",
    "# put in an output layer\n",
    "# activation function is our classification function\n",
    "model_word.add(Dense(units=25375, activation='sigmoid'))\n",
    "\n",
    "model_word.summary()\n",
    "\n",
    "# Compile the model\n",
    "model_word.compile(optimizer='adam',  # You can choose an optimizer (e.g., 'adam', 'sgd')\n",
    "              loss='categorical_crossentropy',  # Specify the loss function for classification\n",
    "              metrics=['accuracy'])  # Optional: Specify metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4946/4952 [============================>.] - ETA: 0s - loss: 6.3131 - accuracy: 0.1486here\n",
      "4952/4952 [==============================] - 52s 11ms/step - loss: 6.3127 - accuracy: 0.1486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d68ddba0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is some example code to train a model with a data generator\n",
    "model_word.fit(x=train_generator, \n",
    "          steps_per_epoch=steps_per_epoch-1,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spooky data model by character for 5 epochs takes ~ 24 min on Felix's computer\n",
    "# with adam optimizer, gets accuracy of 0.3920\n",
    "\n",
    "# spooky data model by word for 5 epochs takes 10 min on Felix's computer\n",
    "# results in accuracy of 0.2110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your trained models so you can re-load instead of re-training each time\n",
    "# also, you'll need these to generate your sentences!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Generate Sentences (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your models if you need to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# # generate a sequence from the model until you get an end of sentence token\n",
    "# This is an example function header you might use\n",
    "# def generate_seq(model: Sequential, \n",
    "#                  tokenizer: Tokenizer, \n",
    "#                  seed: list):\n",
    "#     '''\n",
    "#     Parameters:\n",
    "#         model: your neural network\n",
    "#         tokenizer: the keras preprocessing tokenizer\n",
    "#         seed: [w1, w2, w(n-1)]\n",
    "#     Returns: string sentence\n",
    "#     '''\n",
    "#     pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 points\n",
    "\n",
    "# generate and display one sequence from both the word model and the character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# you may leave _ as _ or replace it with a space if you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 example sentences with each model and save them to a file, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
