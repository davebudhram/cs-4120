{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Classification! (and some n-gram math)\n",
    "\n",
    "9/25/2023, Felix Muzny, Ankit Ramakrishnan, Nidhi Bodar, Harshitha Somala\n",
    "\n",
    "Agenda\n",
    "------\n",
    "+ Detecting the end of a sentence\n",
    "    - Rule-based classifier\n",
    "+ Detecting the sentiment of a sentence\n",
    "    - Rule-based classifier (counting words)\n",
    "    - Measuring Accuracy, Precision, Recall (evaluating a classifier)\n",
    "+ N-gram Math (getting started on things for HW 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead, we'll be focusing on *classification* for much of the next several weeks. Classification can take several forms. Here are some vocabulary terms to get you started:\n",
    "\n",
    "- __classifier__: a model that takes data (text, in NLP) as input and outputs a category\n",
    "- __binary classification__: a model that takes input and outputs *one of two* categories (e.g. \"positive\" or \"negative\")\n",
    "- __multinomial classification__: a model that takes input and outputs *one of many* categories (e.g. \"positive\", \"neutral\" or \"negative\" or a language model that chooses one token from the entire vocabulary)\n",
    "\n",
    "\n",
    "- __rule-based classifier__: a classifier that functions based on rules that humans come up with (e.g. \"the end of a sentence is when there is a \".\" \")\n",
    "- __statistical classifier__: a classifier that functions based on counts (statistics) that it has gathered or based on running an algorithm to automatically train parameters on a given data set. \n",
    "    \n",
    "In this lab, you'll be building rule-based classifiers and evaluating them. We'll learn about our first statistical classifier next lecture\n",
    "\n",
    "All tasks have equal weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0: Who is in your group?\n",
    "\n",
    "Dave Budhram, AkshayDupuguntla, Mario Gonzalez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Detecting the end of a sentence\n",
    "\n",
    "\n",
    "A classifier is, in essence, a function that takes some data $x$ and assigns some label $y$ to it. For a binary classifier, we can model this a function that takes a data point $x$ and returns either `True` or `False`.\n",
    "\n",
    "Later in this class we'll learn about how to build classifiers that automatically learn how to do this, but we'll start where NLP startedâ€”writing some rule-based classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence_end(text: str, target_index: int) -> bool: \n",
    "    \"\"\"\n",
    "    Classify whether or not a *location* is the end of a sentence within\n",
    "    a given text\n",
    "    Parameters:\n",
    "        text - string piece of text\n",
    "        target_index - int candidate location\n",
    "    returns true if the target index is the end of a sentence. \n",
    "    False otherwise. \n",
    "    \"\"\"\n",
    "    # TODO: write a simple, rule-based classifier that\n",
    "    # decides whether or not a specific location is the \n",
    "    # end of a sentence\n",
    "    end_strings = [\".\", \"?\", \"!\"]\n",
    "    return text[target_index] in end_strings\n",
    "\n",
    "# look at the code in the cell below to see example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks were up as advancing issues outpaced declining issues on the NYSE by 1.\n",
      "****\n",
      "5 to 1.\n",
      "****\n",
      " Large- and small-cap stocks were both strong, while the S.\n",
      "****\n",
      "&P.\n",
      "****\n",
      " 500 index gained 0.\n",
      "****\n",
      "46% to finish at 2,457.\n",
      "****\n",
      "59.\n",
      "****\n",
      " Among individual stocks, the two top percentage gainers in the S.\n",
      "****\n",
      "&P.\n",
      "****\n",
      " 500 were Incyte Corporation and Gilead Sciences Inc.\n",
      "****\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example text\n",
    "# feel free to go through different examples\n",
    "\n",
    "# This is the given example text\n",
    "\"\"\"Stocks were up as advancing issues outpaced declining issues \n",
    "          on the NYSE by 1.5 to 1. Large- and small-cap stocks were both strong, \n",
    "          while the S.&P. 500 index gained 0.46% to finish at 2,457.59. Among \n",
    "          individual stocks, the two top percentage gainers in the S.&P. 500 \n",
    "          were Incyte Corporation and Gilead Sciences Inc.\"\"\"\n",
    "\n",
    "example = \"Stocks were up as advancing issues outpaced declining issues on the NYSE by 1.5 to 1. Large- and small-cap stocks were both strong, while the S.&P. 500 index gained 0.46% to finish at 2,457.59. Among individual stocks, the two top percentage gainers in the S.&P. 500 were Incyte Corporation and Gilead Sciences Inc.\"\n",
    "\n",
    "# this code will go through and\n",
    "# build up a string based on the sentence\n",
    "# decisions that your classifier comes up with\n",
    "# it will put \"****\" between the sentences\n",
    "# you do not need to modify any code here\n",
    "so_far = \"\"\n",
    "for index in range(len(example)):\n",
    "    # see how the classify_sentence_end function is called!\n",
    "    result = classify_sentence_end(example, index)\n",
    "    so_far += example[index]\n",
    "    if result:\n",
    "        print(so_far)\n",
    "        print(\"****\")\n",
    "        so_far = \"\"\n",
    "        \n",
    "print(so_far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many sentences are detected using your end of sentence classifier? 10 sentences\n",
    "2. Where did your end of sentence classifier make a mistake? It made a mistake when missing the decimal numbers and abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Determining Sentiment\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use nltk to access the reviews that we want to classify eventually\n",
    "import nltk\n",
    "import nltk.corpus as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_list(filename):\n",
    "    \"\"\"\n",
    "    Loads a lexicon from a plain text file in the format of one word per line.\n",
    "    Parameters:\n",
    "    filename (str): path to file\n",
    "\n",
    "    Returns:\n",
    "    list: list of words\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        # skip the header content\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                break\n",
    "        # read the rest of the lines into a list\n",
    "        return [line.strip() for line in f]\n",
    "    # otherwise return an empty list\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4783\n",
      "2006\n",
      "['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted']\n",
      "['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation']\n"
     ]
    }
   ],
   "source": [
    "# load in the positive and negative word lists here\n",
    "# TODO: the paths to your negative/positive word files here\n",
    "neg_lex = load_word_list(\"./negative_words.txt\")\n",
    "pos_lex = load_word_list(\"./positive_words.txt\")\n",
    "\n",
    "# TODO: How many words are in each list?\n",
    "print(len(neg_lex))\n",
    "print(len(pos_lex))\n",
    "\n",
    "\n",
    "# TODO: Use python's list slicing to look at the first 10 elements in each list\n",
    "neg_first_ten = neg_lex[:10]\n",
    "pos_first_ten = pos_lex[:10]\n",
    "\n",
    "print(neg_first_ten)\n",
    "print(pos_first_ten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['envious', 'enviously', 'enviousness']\n"
     ]
    }
   ],
   "source": [
    "# TODO: which words are in both the positive and the negative lists?\n",
    "in_both_lists = [x for x in neg_lex if x in pos_lex]\n",
    "print(in_both_lists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create our rule-based classifier! We have access to the word lists that you loaded and anything else you know about the world (reflect on how you as a human being can tell if a review is positive/negative). Your classifier need not be perfect, but it should be reasonable (don't just say everything is positive!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_classify(tokens, pos_lexicon, neg_lexicon, verbose = False):\n",
    "    \"\"\"\n",
    "    This function classifies a given tokenized text as positive or negative\n",
    "    based on the provided lexicons.\n",
    "    Parameters:\n",
    "    tokens (list) - list of strings tokenized words in the text to classify\n",
    "    pos_lexicon (list) - list of strings words in the positive word lexicon\n",
    "    neg_lexicon (list) - list of strings words in the negative word lexicon\n",
    "    verbose (boolean) - flag indicating whether or not to print verbose (debugging) output. \n",
    "            Default value False.\n",
    "    Returns:\n",
    "    string \"pos\" if the list of tokens is positive overall, \"neg\" if they are negative overall.\n",
    "    \"\"\"\n",
    "    # TODO: implement this function! This is our classifier.\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in pos_lexicon:\n",
    "            count += 1\n",
    "            if verbose:\n",
    "                print(token + \" is positive\")\n",
    "        if token in neg_lexicon:\n",
    "            count -= 1\n",
    "            if verbose:\n",
    "                print(token + \" is negative\")\n",
    "    if verbose:\n",
    "        print(\"Count so far: \" + str(count))\n",
    "    return \"pos\" if (count >= 0) else \"neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy is positive\n",
      "bastard is negative\n",
      "damn is negative\n",
      "bug is negative\n",
      "virus is negative\n",
      "like is positive\n",
      "flashy is positive\n",
      "likes is positive\n",
      "work is positive\n",
      "wasted is negative\n",
      "well is positive\n",
      "like is positive\n",
      "good is positive\n",
      "like is positive\n",
      "pretty is positive\n",
      "sunken is negative\n",
      "Count so far: 4\n",
      "unexpected is negative\n",
      "scary is negative\n",
      "smart is positive\n",
      "cheesy is negative\n",
      "criticism is negative\n",
      "wonder is positive\n",
      "laughable is negative\n",
      "trash is negative\n",
      "well is positive\n",
      "well is positive\n",
      "intelligent is positive\n",
      "critics is negative\n",
      "complaining is negative\n",
      "lack is negative\n",
      "realistic is positive\n",
      "bright is positive\n",
      "handsome is positive\n",
      "succeed is positive\n",
      "well is positive\n",
      "succeeded is positive\n",
      "perfect is positive\n",
      "cold is negative\n",
      "rocky is negative\n",
      "unfortunately is negative\n",
      "loose is negative\n",
      "creeps is negative\n",
      "shrug is negative\n",
      "blame is negative\n",
      "strong is positive\n",
      "lie is negative\n",
      "led is positive\n",
      "respectable is positive\n",
      "attack is negative\n",
      "wise is positive\n",
      "stronger is positive\n",
      "stronger is positive\n",
      "love is positive\n",
      "strong is positive\n",
      "intelligence is positive\n",
      "proven is positive\n",
      "effectively is positive\n",
      "well is positive\n",
      "balanced is positive\n",
      "plot is negative\n",
      "useful is positive\n",
      "well is positive\n",
      "smart is positive\n",
      "cliched is negative\n",
      "logical is positive\n",
      "flaws is negative\n",
      "lying is negative\n",
      "mediocre is negative\n",
      "bad is negative\n",
      "believable is positive\n",
      "homage is positive\n",
      "realistic is positive\n",
      "plot is negative\n",
      "dislike is negative\n",
      "excuse is negative\n",
      "disagree is negative\n",
      "destroy is negative\n",
      "gripe is negative\n",
      "picky is negative\n",
      "beautiful is positive\n",
      "good is positive\n",
      "improvement is positive\n",
      "bad is negative\n",
      "good is positive\n",
      "like is positive\n",
      "good is positive\n",
      "mistake is negative\n",
      "top is positive\n",
      "annoying is negative\n",
      "distracting is negative\n",
      "limited is negative\n",
      "impressive is positive\n",
      "talent is positive\n",
      "fun is positive\n",
      "better is positive\n",
      "best is positive\n",
      "annoying is negative\n",
      "appealing is positive\n",
      "nice is positive\n",
      "better is positive\n",
      "incapable is negative\n",
      "enjoyable is positive\n",
      "effective is positive\n",
      "like is positive\n",
      "well is positive\n",
      "strong is positive\n",
      "hell is negative\n",
      "won is positive\n",
      "scary is negative\n",
      "mediocre is negative\n",
      "wonderful is positive\n",
      "scariest is negative\n",
      "scream is negative\n",
      "strong is positive\n",
      "smart is positive\n",
      "intelligent is positive\n",
      "entertaining is positive\n",
      "Count so far: 15\n",
      "positive review is pos\n",
      "negative review is pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/davebudhram/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# now, we'll test out your classifier!\n",
    "# Here are two example movie reviews.\n",
    "nltk.download('movie_reviews')\n",
    "movies = corpus.movie_reviews\n",
    "\n",
    "# load in a single negative review\n",
    "negative_toks = movies.words('neg/cv001_19502.txt')\n",
    "# uncomment the text below to see the contents of the review\n",
    "# neg_text = \" \".join(negative_toks)\n",
    "# print(neg_text)\n",
    "\n",
    "# load in a single positive review\n",
    "positive_toks = movies.words('pos/cv992_11962.txt')\n",
    "# pos_text = \" \".join(positive_toks)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# call your rule_based_classify on these example reviews.\n",
    "first_classifier = rule_based_classify(negative_toks, pos_lex, neg_lex, True)\n",
    "second_classifier = rule_based_classify(positive_toks, pos_lex, neg_lex, True)\n",
    "\n",
    "print(\"positive review is \" + first_classifier)\n",
    "print(\"negative review is \" + second_classifier)\n",
    "\n",
    "# Does our classification function label them correctly? Why or why not?\n",
    "# take a look at the contents of the reviews\n",
    "# The classifier function does not label them correctly because the negative review contains a lot of seemingly positive words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What labels does your classifier assign these two reviews? The classifier assigned both these reviews to be positive\n",
    "2. Are these correct? This is incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: How good is your sentiment classifier?\n",
    "-----\n",
    "\n",
    "Given the movies dataset from `nltk`, how many of the reviews does your classifier classify correctly?\n",
    "\n",
    "We'll look at three different metrics: __accuracy__, __precision__, and __recall__.\n",
    "\n",
    "__accuracy__: what you think of when you think of correctness.\n",
    "$$ \\frac{\\texttt{number correct}}{\\texttt{total number}}$$\n",
    "\n",
    "Precision and recall require differentiated between the ways in which the classifier can be correct or incorrect. \n",
    "\n",
    "- __true positive__: an example whose gold label is positive and that the classifier labels as positive\n",
    "- __true negative__: an example whose gold label is negative and that the classifier labels as negative\n",
    "- __false positive__: an example whose gold label is negative and that the classifier labels as positive\n",
    "- __false negative__: an example whose gold label is positive and that the classifier labels as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# you can use numpy's random functionality if you'd like to\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['neg/cv114_19501.txt', 'neg/cv033_25680.txt', 'neg/cv683_13047.txt', 'neg/cv545_12848.txt', 'neg/cv435_24355.txt', 'neg/cv566_8967.txt', 'neg/cv976_10724.txt', 'neg/cv787_15277.txt', 'neg/cv918_27080.txt', 'neg/cv989_17297.txt', 'neg/cv254_5870.txt', 'neg/cv039_5963.txt', 'neg/cv151_17231.txt', 'neg/cv280_8651.txt', 'neg/cv669_24318.txt', 'neg/cv916_17034.txt', 'neg/cv759_15091.txt', 'neg/cv287_17410.txt', 'neg/cv187_14112.txt', 'neg/cv451_11502.txt', 'neg/cv092_27987.txt', 'neg/cv681_9744.txt', 'neg/cv093_15606.txt', 'neg/cv324_7502.txt', 'neg/cv906_12332.txt', 'neg/cv360_8927.txt', 'neg/cv822_21545.txt', 'neg/cv407_23928.txt', 'neg/cv196_28898.txt', 'neg/cv538_28485.txt', 'neg/cv903_18981.txt', 'neg/cv215_23246.txt', 'neg/cv497_27086.txt', 'neg/cv181_16083.txt', 'neg/cv632_9704.txt', 'neg/cv733_9891.txt', 'neg/cv496_11185.txt', 'neg/cv973_10171.txt', 'neg/cv115_26443.txt', 'neg/cv138_13903.txt', 'neg/cv946_20084.txt', 'neg/cv300_23302.txt', 'neg/cv426_10976.txt', 'neg/cv957_9059.txt', 'neg/cv750_10606.txt', 'neg/cv439_17633.txt', 'neg/cv356_26170.txt', 'neg/cv313_19337.txt', 'neg/cv829_21725.txt', 'neg/cv344_5376.txt', 'neg/cv383_14662.txt', 'neg/cv434_5641.txt', 'neg/cv830_5778.txt', 'neg/cv329_29293.txt', 'neg/cv460_11723.txt', 'neg/cv988_20168.txt', 'neg/cv698_16930.txt', 'neg/cv852_27512.txt', 'neg/cv143_21158.txt', 'neg/cv490_18986.txt', 'neg/cv255_15267.txt', 'neg/cv102_8306.txt', 'neg/cv725_10266.txt', 'neg/cv552_0150.txt', 'neg/cv634_11989.txt', 'neg/cv476_18402.txt', 'neg/cv883_27621.txt', 'neg/cv405_21868.txt', 'neg/cv088_25274.txt', 'neg/cv986_15092.txt', 'neg/cv265_11625.txt', 'neg/cv528_11669.txt', 'neg/cv687_22207.txt', 'neg/cv840_18033.txt', 'neg/cv878_17204.txt', 'neg/cv508_17742.txt', 'neg/cv774_15488.txt', 'neg/cv080_14899.txt', 'neg/cv864_3087.txt', 'neg/cv268_20288.txt', 'neg/cv267_16618.txt', 'neg/cv048_18380.txt', 'neg/cv833_11961.txt', 'neg/cv489_19046.txt', 'neg/cv220_28906.txt', 'neg/cv715_19246.txt', 'neg/cv679_28221.txt', 'neg/cv132_5423.txt', 'neg/cv847_20855.txt', 'neg/cv919_18155.txt', 'neg/cv665_29386.txt', 'neg/cv038_9781.txt', 'neg/cv890_3515.txt', 'neg/cv522_5418.txt', 'neg/cv112_12178.txt', 'neg/cv843_17054.txt', 'neg/cv010_29063.txt', 'neg/cv965_26688.txt', 'neg/cv975_11920.txt', 'neg/cv945_13012.txt']\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# To see the available file ids, this is one way that we can access them.\n",
    "# This will give you a list of neg/positive file ids.\n",
    "print(len(movies.fileids('neg')))\n",
    "# choose 100 random items without replacement from a list\n",
    "print(random.sample(movies.fileids('neg'), 100))\n",
    "print(len(movies.fileids('pos')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive counts:62\n",
      "False positive counts:28\n",
      "False negative counts:38\n",
      "True negative counts:72\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Write code that uses your classifier to classify 100 randomly chosen\n",
    "# negative reviews and 100 randomly chosen positive reviews\n",
    "# count the number of true positives, true negatives, false positives, and false negatives\n",
    "\n",
    "# to get the tokens associated with a certain file id,\n",
    "# tokens = movies.words(file_id)\n",
    "\n",
    "# takes a long time to run if you loop over all fileids as opposed to just\n",
    "# 100 randomly chosen ones\n",
    "# make sure you don't classify the same review twice!\n",
    "# (it takes us about 10 seconds to classify 200 reviews on a 2020 macbook air)\n",
    "pos_list = random.sample(movies.fileids('pos'), 100)\n",
    "neg_list = random.sample(movies.fileids('neg'), 100)\n",
    "\n",
    "true_postive_count = 0\n",
    "true_negative_count = 0\n",
    "false_postive_count = 0\n",
    "false_negative_count = 0\n",
    "\n",
    "for file in pos_list:\n",
    "    tokens = movies.words(file)\n",
    "    result = rule_based_classify(tokens, pos_lex, neg_lex)\n",
    "    if (result == \"pos\"):\n",
    "        true_postive_count += 1\n",
    "    else:\n",
    "        false_negative_count += 1\n",
    "\n",
    "\n",
    "for file in neg_list:\n",
    "    tokens = movies.words(file)\n",
    "    rule_based_classify(tokens, pos_lex, neg_lex)\n",
    "    result = rule_based_classify(tokens, pos_lex, neg_lex)\n",
    "    if (result == \"neg\"):\n",
    "        true_negative_count += 1\n",
    "    else:\n",
    "        false_postive_count += 1\n",
    "\n",
    "\n",
    "    \n",
    "# TODO: print out the number of true positives, false positives,\n",
    "# false negatives, and true negatives\n",
    "\n",
    "print(\"True positive counts:\" + str(true_postive_count))\n",
    "print(\"False positive counts:\" + str(false_postive_count))\n",
    "print(\"False negative counts:\" + str(false_negative_count))\n",
    "print(\"True negative counts:\" + str(true_negative_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the equations for accuracy, precision, and recall in terms of what we've just been counting. $tp$ means true positive, $fp$ means false positive, $fn$ means false negative, and $tn$ means true negative.\n",
    "\n",
    "$$ accuracy = \\frac{tp + tn}{tp + fp + fn + tn}$$\n",
    "\n",
    "$$ precision = \\frac{tp}{tp + fp}$$\n",
    "\n",
    "$$ recall = \\frac{tp}{tp + fn}$$\n",
    "\n",
    "You can think of precision as \"how many of my positive guesses were correct?\" and recall as \"how many of the positive examples did I find?\" ðŸ˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate and print accuracy\n",
    "accuracy = (true_postive_count + true_negative_count) / (true_postive_count + false_postive_count + false_negative_count + true_negative_count)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6888888888888889\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate and print precision\n",
    "precision = (true_postive_count) / (true_postive_count + false_postive_count)\n",
    "print(precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate and print recall\n",
    "recall = (true_postive_count) / (true_postive_count + false_negative_count)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: n-gram math\n",
    "----\n",
    "\n",
    "Your final task in this lab is to do some math that will help you with your n-gram language model homework. Remember in HW 1 how you implemented a `count_list` function? Some of you were clever with how you implemented it, but let's look at a less clever implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took: 0.029832124710083008 seconds!\n",
      "That took: 0.0001220703125 seconds!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "def count_list(ls: list) -> dict:\n",
    "    counts = {}\n",
    "    for item in ls:\n",
    "        # we're not going to be clever about counting here,\n",
    "        # no conditionals, no sets, nothing\n",
    "        counts[item] = ls.count(item)\n",
    "    return counts\n",
    "\n",
    "# see the difference between the following two items\n",
    "example = [random.randint(0, 100) for i in range(2000)]\n",
    "start = time.time()\n",
    "count_list(example)\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds!\")\n",
    "\n",
    "# this takes a very similar amount of time to count_dict from HW 1\n",
    "start = time.time()\n",
    "Counter(example)\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put your create_ngrams (or make_ngrams) function here!\n",
    "def make_ngrams(tokens: list, n: int) -> list:\n",
    "    \"\"\"Creates n-grams for the given token sequence.\n",
    "    Args:\n",
    "    tokens (list): a list of tokens as strings\n",
    "    n (int): the length of n-grams to create\n",
    "\n",
    "    Returns:\n",
    "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "    \"\"\"\n",
    "    # TODO: implement this function!\n",
    "    ans = []\n",
    "    length = len(tokens)\n",
    "    for i in range (length - n + 1):\n",
    "        tuple = ()\n",
    "        for j in range (n):\n",
    "            tuple += (tokens[i + j],)\n",
    "        ans.append(tuple)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final probability: 0.3333333333333333\n",
      "That took 7.414817810058594e-05 seconds!\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate the bigram score of the following sequence of tokens\n",
    "# for this example, we'll use a \"vanilla\" scoring technique\n",
    "# no Laplace smoothing, no unknown tokens\n",
    "training_data = [\"<s>\", \"I\", \"love\", \"dogs\", \"</s>\", \"<s>\", \"I\", \"love\", \"cats\", \"</s>\", \"<s>\", \"I\", \"love\", \"dinosaurs\", \"</s>\"]\n",
    "\n",
    "# TODO: call your create_ngrams function to get your bigrams\n",
    "bigrams = make_ngrams(training_data, 2)\n",
    "count_bigrams = count_list(bigrams)\n",
    "\n",
    "\n",
    "\n",
    "to_score = [\"<s>\", \"I\", \"love\", \"cats\", \"</s>\"]\n",
    "start = time.time()\n",
    "training_data_counts = count_list(training_data)\n",
    "\n",
    "# BEGIN SCORING SECTION\n",
    "# start probability at one so that we can multiply the probability of\n",
    "# each subsequent next token with it\n",
    "total_prob = 1\n",
    "for i in range(1, len(to_score)):\n",
    "    # TODO: YOUR SCORE CALCULATION CODE HERE\n",
    "    curr_count = training_data_counts[to_score[i - 1]]\n",
    "    curr_bigram = (to_score[i-1], to_score[i])\n",
    "    bigram_count = count_bigrams[curr_bigram]\n",
    "    total_prob *= bigram_count / curr_count\n",
    "\n",
    "    \n",
    "\n",
    "# END SCORING SECTION\n",
    "end = time.time()\n",
    "\n",
    "# print your final probability\n",
    "print(\"Final probability:\", total_prob)\n",
    "print(\"That took\", end - start, \"seconds!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tokens: 56670\n",
      "That took 0.00432586669921875 seconds!\n"
     ]
    }
   ],
   "source": [
    "# Finally, pretend that we had a lot more data\n",
    "training_data = [\"<s>\", \"I\", \"love\", \"dogs\", \"</s>\", \"<s>\", \"I\", \"love\", \"cats\", \"</s>\", \"<s>\", \"I\", \"love\", \"dinosaurs\", \"</s>\"]\n",
    "# this is the amount of training data in the berp set\n",
    "training_data = training_data * 3778\n",
    "\n",
    "# TODO: call your create_ngrams function here\n",
    "bigrams2 = make_ngrams(training_data, 2)\n",
    "\n",
    "\n",
    "print(\"Number of training tokens:\", len(training_data))\n",
    "start = time.time()\n",
    "# and what if we had 5000 sentences to score?\n",
    "for example_num in range(3000):\n",
    "    # TODO: COPY AND PASTE YOUR SCORING CODE HERE (between \"BEGIN SCORING SECTION\" and \"END SCORING SECTION\")\n",
    "    # (remove any print statements that you have)\n",
    "    # (make sure it is appropriately indented)\n",
    "    total_prob = 1\n",
    "    for i in range(1, len(to_score)):\n",
    "        # TODO: YOUR SCORE CALCULATION CODE HERE\n",
    "        curr_count = training_data_counts[to_score[i - 1]]\n",
    "        curr_bigram = (to_score[i-1], to_score[i])\n",
    "        bigram_count = count_bigrams[curr_bigram]\n",
    "        total_prob *= bigram_count / curr_count\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(\"That took\", end - start, \"seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the moral of the story? If you perform your counts at the same time you score, you'll be doing the same work over and over again which will result in a significantly slower model!\n",
    "\n",
    "Make sure that you're gathering the counts that you need in `train` and only performing scoring calculations (as opposed to also counting things) in `score`.\n",
    "\n",
    "This is particularly important when using larger data sets! (berp is not that big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
